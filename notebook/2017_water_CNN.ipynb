{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layering: divide the data into N layers, make sure every layer has the same distribution of 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layering(df, N):\n",
    "    new_data=df.iloc[:,0:]\n",
    "\n",
    "    data_maj = new_data[new_data['EVENT']==0]\n",
    "    data_min = new_data[new_data['EVENT']==1]\n",
    "    n_maj=data_maj.iloc[:,0].size\n",
    "    n_min=data_min.iloc[:,0].size\n",
    "    M1=n_maj%N\n",
    "    M2=n_min%N\n",
    "    stepD=int(n_maj/10)\n",
    "    stepS=int(n_min/10)\n",
    "\n",
    "    maj_data = []\n",
    "    for i in range(N):\n",
    "        maj_data.append(data_maj.iloc[i*stepD:(i+1)*stepD])\n",
    "    for i in range(M1):\n",
    "        maj_data[i]=maj_data[i].append(data_maj.iloc[stepD*N+i:stepD*N+i+1])\n",
    "\n",
    "\n",
    "    min_data = []\n",
    "    for i in range(N):\n",
    "        min_data.append(data_min.iloc[i*stepS:(i+1)*stepS])\n",
    "    for i in range(M2):\n",
    "        min_data[i]=min_data[i].append(data_min.iloc[stepS*N+i:stepS*N+i+1])\n",
    "\n",
    "    Last_Data = pd.DataFrame()\n",
    "    for i in range(N):\n",
    "        Last_Data=Last_Data.append(maj_data[i].append(min_data[i]))\n",
    "    return Last_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### somte sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smoter(X, y, is_random=False):\n",
    "    if is_random == True:\n",
    "        random_lst = list(np.random.randint(0, 1000, 4))\n",
    "    elif is_random == False:\n",
    "        random_lst = [0] * 4\n",
    "\n",
    "    sm = SMOTE(random_state=random_lst[2])\n",
    "    X_smote, y_smote = sm.fit_sample(X, y)\n",
    "    y_smote = y_smote[:,np.newaxis]\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(v_xs, v_ys, sess):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob:1}) # y_pre 是一个 n_samples*2 的概率向量\n",
    "    \n",
    "    predicted = np.argmax(y_pre, 1)\n",
    "    actual = np.argmax(v_ys, 1)\n",
    "    \n",
    "#     print(\"predicted\", predicted)\n",
    "#     print(\"actual\", actual)\n",
    "    \n",
    "    # Count true positives, true negatives, false positives and false negatives.\n",
    "    tp = np.count_nonzero(predicted * actual)\n",
    "    tn = np.count_nonzero((predicted - 1) * (actual - 1))\n",
    "    fp = np.count_nonzero(predicted * (actual - 1))\n",
    "    fn = np.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "#     print('TP=',tp,'FP=',fp,'TN=',tn,'FN=',fn)\n",
    "    # Calculate accuracy, precision, recall and F1 score.\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "#     print('Precision = ', precision)\n",
    "#     print('Recall = ', recall)\n",
    "#     print('F1 Score = ', f1_score)\n",
    "#     print('Accuracy = ', accuracy)\n",
    "    return precision, recall, f1_score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size = 100):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    if name is None:\n",
    "        return tf.Variable(initial)\n",
    "    else:\n",
    "        return tf.Variable(initial,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    if name is None:\n",
    "        return tf.Variable(initial)\n",
    "    else:\n",
    "        return tf.Variable(initial,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(x, W):\n",
    "    return tf.nn.conv1d(x, W, stride=stride_num, padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = 1\n",
    "stride_num = 1\n",
    "pool_patch_size = 2\n",
    "kf = 100\n",
    "lr = 0.001\n",
    "max_iterations = 100\n",
    "hidden_cell_num = 64\n",
    "random_seed = 42\n",
    "valid_size = 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load 2017 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('../data/water/txt/2017waterDataTraining.txt',delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "Time = np.zeros(df.shape[0]).astype(\"str\")\n",
    "for i in range(len(df)):\n",
    "    Time[i] = df['index'][i]+\" \"+ df['Time'][i]\n",
    "df['Time'] = Time\n",
    "df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering\n",
    "\n",
    "\n",
    "It looks like we have 14 columns to help us predict our classification. We will drop fnlwgt and education and then convert our categorical features to dummy variables. We will also convert our label to 0 and 1 where 1 means the person made more than $50k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Time']\n",
    "continuous_features = ['Tp', 'Cl', 'pH', 'Redox', 'Leit', 'Trueb', 'Cl_2', 'Fm', 'Fm_2']\n",
    "cat_features =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_dummies = pd.get_dummies(df, columns=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_dummies.drop(drop_columns, 1, inplace=True)\n",
    "# delte NA datas\n",
    "all_df_dummies = all_df_dummies.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_df_dummies.drop(['EVENT'], axis=1) # Series\n",
    "y_train = all_df_dummies['EVENT'].apply(lambda x: 0 if x == False else 1) # Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train,y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tp</th>\n",
       "      <th>Cl</th>\n",
       "      <th>pH</th>\n",
       "      <th>Redox</th>\n",
       "      <th>Leit</th>\n",
       "      <th>Trueb</th>\n",
       "      <th>Cl_2</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Fm_2</th>\n",
       "      <th>EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.110</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1471.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.37</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tp    Cl    pH  Redox   Leit  Trueb   Cl_2      Fm    Fm_2  EVENT\n",
       "0  4.4  0.14  8.38  755.0  232.0  0.009  0.110  1428.0  1020.0      0\n",
       "1  4.4  0.14  8.38  755.0  232.0  0.009  0.111  1436.0  1018.0      0\n",
       "2  4.4  0.14  8.38  755.0  232.0  0.014  0.113  1471.0  1019.0      0\n",
       "3  4.4  0.14  8.37  755.0  232.0  0.015  0.111  1457.0  1015.0      0\n",
       "4  4.4  0.14  8.38  755.0  232.0  0.013  0.111  1476.0  1019.0      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ layer sampling ============\n"
     ]
    }
   ],
   "source": [
    "print(\"============ layer sampling ============\")\n",
    "train_layer = Layering(train, kf)\n",
    "array = train_layer.values\n",
    "X_train = array[:, 0:-1] # ndarray\n",
    "y_train = array[:, -1] # ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_vld, y_tr, y_vld = train_test_split(X_train, y_train, test_size=valid_size,\n",
    "                                                stratify = y_train, random_state = random_seed)\n",
    "# stratify： 按正负样本原始比例random_seed分配给train 和 valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do somte sampling on the train data to solve data imblance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ SMOTE ============\n",
      "train: 110812, contains 0.9843 of 0 , after SMOTE: train: 146156 contains 0.5000 of 1\n"
     ]
    }
   ],
   "source": [
    "X_train_oversampled, y_train_oversampled = Smoter(X_tr, y_tr, is_random=True)\n",
    "print(\"============ SMOTE ============\")\n",
    "print(\"train: %d, contains %.4f of 0 , after SMOTE: train: %d contains %.4f of 1\" %(X_train.shape[0], (y_train == 0).sum()/y_train.shape[0], X_train_oversampled.shape[0], (y_train_oversampled == 0).sum()/y_train_oversampled.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize the train and valid\n",
    "\n",
    "fulfill the Na with median, then standardized the data, output type ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])\n",
    "X_train_oversampled = clean_pipeline.fit_transform(X_train_oversampled)\n",
    "X_vld = clean_pipeline.fit_transform(X_vld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfer y into probability vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oversampled_pro = np.zeros([y_train_oversampled.shape[0], 2])\n",
    "for i in range(len(y_train_oversampled)):\n",
    "    if y_train_oversampled[i] == 1:\n",
    "        y_train_oversampled_pro[i] = np.array([0, 1])\n",
    "    else:\n",
    "        y_train_oversampled_pro[i] = np.array([1, 0])\n",
    "y_train_oversampled = y_train_oversampled_pro    \n",
    "\n",
    "y_vld_pro = np.zeros([y_vld.shape[0], 2])\n",
    "for i in range(len(y_vld)):\n",
    "    if y_vld[i] == 1:\n",
    "        y_vld_pro[i] = np.array([0, 1])\n",
    "    else:\n",
    "        y_vld_pro[i] = np.array([1, 0])\n",
    "y_vld = y_vld_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load 2017 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"../data/water/txt/2017waterDataTesting.txt\").readlines()\n",
    "num_lines = len(lines) - 1\n",
    "\n",
    "X_test = np.ones((num_lines, 9))\n",
    "y_test = np.ones((num_lines, 1))\n",
    "flag = 0\n",
    "\n",
    "lines = np.delete(lines, 0, axis = 0)\n",
    "i = 0\n",
    "\n",
    "for line in lines:\n",
    "    data_line = line.split()\n",
    "    feature = data_line[3:12]\n",
    "    for k in range(9):\n",
    "        if feature[k] == 'NA':\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "        flag = 0\n",
    "        continue    # jump out of the loop\n",
    "    X_test[i] = feature    \n",
    "    if data_line[12] == 'FALSE':\n",
    "        y_test[i] = 0\n",
    "    elif data_line[12] == 'TRUE':\n",
    "        y_test[i] = 1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "X_test = clean_pipeline.transform(X_test) \n",
    "\n",
    "y_test_pro = np.zeros([y_test.shape[0], 2])\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 1:\n",
    "        y_test_pro[i] = np.array([0, 1])\n",
    "    else:\n",
    "        y_test_pro[i] = np.array([1, 0])\n",
    "y_test = y_test_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: 1 contains:  60950 / 244668\n"
     ]
    }
   ],
   "source": [
    "y_test_nonezero = np.count_nonzero(np.argmax(y_test, 1))\n",
    "print(\"y_test: 1 contains: \", y_test_nonezero, \"/\",len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造网络\n",
    "两个（卷积+最大池化），两个全联接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define placeholder for inputs to network\n",
    "# keep_prob = tf.placeholder(tf.float32)\n",
    "# xs = tf.placeholder(tf.float32, [None, 9])\n",
    "# ys = tf.placeholder(tf.float32, [None, 2])\n",
    "# learning_rate = tf.placeholder(tf.float32)\n",
    "# X_input = tf.reshape(xs,[-1,9,1]) # [n_samples, 9 ,1]    -1 具体是多少由导入数据决定（多少组数据） \n",
    "    \n",
    "# def my_cnn():\n",
    "    \n",
    "#    ## conv1 layer ##\n",
    "#     W_conv1 = weight_variable([3,1,6],name=\"W_conv1\") # patch: 3, in size 1(通道数), out size 6（feature_map数量，一个卷积核生成一个feature_map）\n",
    "#     b_conv1 = bias_variable([6],name=\"b_conv1\")\n",
    "#     h_conv1 = tf.nn.relu(conv1d(X_input, W_conv1) + b_conv1) # output size 1*9*6\n",
    "#     print(h_conv1.shape)\n",
    "\n",
    "#     ## conv2 layer ##\n",
    "#     W_conv2 = weight_variable([3,6,12],name=\"W_conv2\") # patch: 3, in size 6，out size 12\n",
    "#     b_conv2 = bias_variable([12],name=\"b_conv2\")\n",
    "#     h_conv2 = tf.nn.relu(conv1d(h_conv1, W_conv2) + b_conv2) # output size 1*9*12\n",
    "#     print(h_conv2.shape)\n",
    "\n",
    "#     ## func1 layer ##\n",
    "#     W_fc1 = weight_variable([9*12,hidden_cell_num],name=\"W_fc1\")\n",
    "#     b_fc1 = bias_variable([hidden_cell_num],name=\"b_fc1\")\n",
    "\n",
    "#     h_conv2_flat = tf.reshape(h_conv2, [-1,9*12])\n",
    "#     h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1)+b_fc1)\n",
    "#     h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#     print(h_fc1_drop.shape)\n",
    "\n",
    "#     ## func2 layer ##\n",
    "#     W_fc2 = weight_variable([hidden_cell_num,2],name=\"W_fc2\")\n",
    "#     b_fc2 = bias_variable([1],name=\"b_fc2\")\n",
    "#     prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "#     print(prediction.shape)\n",
    "\n",
    "#     var_dict = {'W_conv1': W_conv1, \n",
    "#                 'b_conv1': b_conv1, \n",
    "#                 'W_conv2': W_conv2, \n",
    "#                 'b_conv2': b_conv2, \n",
    "#                 'W_fc1': W_fc1, \n",
    "#                 'b_fc1': b_fc1,\n",
    "#                 'W_fc2': W_fc2,\n",
    "#                 'b_fc2': b_fc2}\n",
    "#     return prediction, var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder for inputs to network\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "xs = tf.placeholder(tf.float32, [None, 9])\n",
    "ys = tf.placeholder(tf.float32, [None, 2])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "X_input = tf.reshape(xs,[-1,9,1]) # [n_samples, 9 ,1]    -1 具体是多少由导入数据决定（多少组数据） \n",
    "    \n",
    "def my_cnn():\n",
    "    \n",
    "    ## conv1 layer ##\n",
    "    W_conv1 = weight_variable([3,1,6],name=\"W_conv1\") # patch: 3, in size 1(通道数), out size 6（feature_map数量，一个卷积核生成一个feature_map）\n",
    "    b_conv1 = bias_variable([6],name=\"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(conv1d(X_input, W_conv1) + b_conv1) # output size 1*9*6\n",
    "    print(\"h_conv1\", h_conv1.shape)\n",
    "    ## poo1 layer ##\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=h_conv1, pool_size=pool_patch_size, strides=stride_num, padding='same')\n",
    "    print(\"max_pool_1\", max_pool_1.shape)\n",
    "\n",
    "    ## conv2 layer ##\n",
    "    W_conv2 = weight_variable([3,6,12],name=\"W_conv2\") # patch: 3, in size 6，out size 12\n",
    "    b_conv2 = bias_variable([12],name=\"b_conv2\")\n",
    "    h_conv2 = tf.nn.relu(conv1d(max_pool_1, W_conv2) + b_conv2) # output size 1*9*12\n",
    "    print(\"h_conv2\", h_conv2.shape)\n",
    "    ## poo2 layer ##\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=h_conv2, pool_size=pool_patch_size, strides=stride_num, padding='same')\n",
    "    print(\"max_pool_2\", max_pool_2.shape)\n",
    "\n",
    "    ## func1 layer ##\n",
    "    W_fc1 = weight_variable([9*12,hidden_cell_num],name=\"W_fc1\")\n",
    "    b_fc1 = bias_variable([hidden_cell_num],name=\"b_fc1\")\n",
    "\n",
    "    max_pool_2_flat = tf.reshape(max_pool_2, [-1,9*12])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(max_pool_2_flat, W_fc1)+b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    print(\"h_fc1_drop\", h_fc1_drop.shape)\n",
    "\n",
    "    ## func2 layer ##\n",
    "    W_fc2 = weight_variable([hidden_cell_num,2],name=\"W_fc2\")\n",
    "    b_fc2 = bias_variable([1],name=\"b_fc2\")\n",
    "    prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    print(\"prediction\", prediction.shape)\n",
    "\n",
    "    var_dict = {'W_conv1': W_conv1, \n",
    "                'b_conv1': b_conv1, \n",
    "                'W_conv2': W_conv2, \n",
    "                'b_conv2': b_conv2, \n",
    "                'W_fc1': W_fc1, \n",
    "                'b_fc1': b_fc1,\n",
    "                'W_fc2': W_fc2,\n",
    "                'b_fc2': b_fc2}\n",
    "    return prediction, var_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_conv1 (?, 9, 6)\n",
      "max_pool_1 (?, 9, 6)\n",
      "h_conv2 (?, 9, 12)\n",
      "max_pool_2 (?, 9, 12)\n",
      "h_fc1_drop (?, 64)\n",
      "prediction (?, 2)\n"
     ]
    }
   ],
   "source": [
    "prediction, var_dict = my_cnn()\n",
    "\n",
    "# the error between prediction and real data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=ys))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/100\n",
      " Train loss: 0.692531 Train acc: 0.491044 Train f1: 0.644107\n",
      " Valid loss: 0.705584 Valid acc: 0.105229 Valid f1: 0.027637\n",
      "Iteration: 2/100\n",
      " Train loss: 0.691024 Train acc: 0.509962 Train f1: 0.663028\n",
      " Valid loss: 0.709121 Valid acc: 0.076269 Valid f1: 0.029311\n",
      "Iteration: 3/100\n",
      " Train loss: 0.689598 Train acc: 0.520649 Train f1: 0.671438\n",
      " Valid loss: 0.712352 Valid acc: 0.060435 Valid f1: 0.030312\n",
      "Iteration: 4/100\n",
      " Train loss: 0.688187 Train acc: 0.530495 Train f1: 0.676479\n",
      " Valid loss: 0.715417 Valid acc: 0.050645 Valid f1: 0.030388\n",
      "Iteration: 5/100\n",
      " Train loss: 0.686778 Train acc: 0.538329 Train f1: 0.679348\n",
      " Valid loss: 0.718109 Valid acc: 0.047090 Valid f1: 0.030709\n",
      "Iteration: 6/100\n",
      " Train loss: 0.685425 Train acc: 0.545691 Train f1: 0.680290\n",
      " Valid loss: 0.720246 Valid acc: 0.048758 Valid f1: 0.030978\n",
      "Iteration: 7/100\n",
      " Train loss: 0.684102 Train acc: 0.553149 Train f1: 0.679432\n",
      " Valid loss: 0.721390 Valid acc: 0.055732 Valid f1: 0.031090\n",
      "Iteration: 8/100\n",
      " Train loss: 0.682766 Train acc: 0.563726 Train f1: 0.677758\n",
      " Valid loss: 0.721534 Valid acc: 0.070389 Valid f1: 0.031455\n",
      "Iteration: 9/100\n",
      " Train loss: 0.681370 Train acc: 0.577034 Train f1: 0.675666\n",
      " Valid loss: 0.720764 Valid acc: 0.092567 Valid f1: 0.031634\n",
      "Iteration: 10/100\n",
      " Train loss: 0.679914 Train acc: 0.586298 Train f1: 0.666523\n",
      " Valid loss: 0.719164 Valid acc: 0.127817 Valid f1: 0.032225\n",
      "Iteration: 11/100\n",
      " Train loss: 0.678406 Train acc: 0.593797 Train f1: 0.650131\n",
      " Valid loss: 0.716945 Valid acc: 0.168809 Valid f1: 0.033023\n",
      "Iteration: 12/100\n",
      " Train loss: 0.676847 Train acc: 0.596240 Train f1: 0.621854\n",
      " Valid loss: 0.714359 Valid acc: 0.212043 Valid f1: 0.033930\n",
      "Iteration: 13/100\n",
      " Train loss: 0.675240 Train acc: 0.603013 Train f1: 0.596296\n",
      " Valid loss: 0.711647 Valid acc: 0.255086 Valid f1: 0.034864\n",
      "Iteration: 14/100\n",
      " Train loss: 0.673584 Train acc: 0.616116 Train f1: 0.583037\n",
      " Valid loss: 0.709020 Valid acc: 0.297227 Valid f1: 0.035431\n",
      "Iteration: 15/100\n",
      " Train loss: 0.671877 Train acc: 0.630108 Train f1: 0.575345\n",
      " Valid loss: 0.706632 Valid acc: 0.335184 Valid f1: 0.035928\n",
      "Iteration: 16/100\n",
      " Train loss: 0.670118 Train acc: 0.641041 Train f1: 0.570045\n",
      " Valid loss: 0.704572 Valid acc: 0.367808 Valid f1: 0.037552\n",
      "Iteration: 17/100\n",
      " Train loss: 0.668312 Train acc: 0.650572 Train f1: 0.567272\n",
      " Valid loss: 0.702785 Valid acc: 0.394115 Valid f1: 0.038285\n",
      "Iteration: 18/100\n",
      " Train loss: 0.666474 Train acc: 0.657147 Train f1: 0.564405\n",
      " Valid loss: 0.701363 Valid acc: 0.414762 Valid f1: 0.039582\n",
      "Iteration: 19/100\n",
      " Train loss: 0.664586 Train acc: 0.660076 Train f1: 0.559806\n",
      " Valid loss: 0.700399 Valid acc: 0.431197 Valid f1: 0.040590\n",
      "Iteration: 20/100\n",
      " Train loss: 0.662652 Train acc: 0.662354 Train f1: 0.557126\n",
      " Valid loss: 0.699975 Valid acc: 0.438170 Valid f1: 0.040984\n",
      "Iteration: 21/100\n",
      " Train loss: 0.660672 Train acc: 0.663832 Train f1: 0.555916\n",
      " Valid loss: 0.700182 Valid acc: 0.441452 Valid f1: 0.041215\n",
      "Iteration: 22/100\n",
      " Train loss: 0.658651 Train acc: 0.664591 Train f1: 0.555751\n",
      " Valid loss: 0.701109 Valid acc: 0.439647 Valid f1: 0.041267\n",
      "Iteration: 23/100\n",
      " Train loss: 0.656591 Train acc: 0.665118 Train f1: 0.556742\n",
      " Valid loss: 0.702792 Valid acc: 0.433658 Valid f1: 0.040937\n",
      "Iteration: 24/100\n",
      " Train loss: 0.654500 Train acc: 0.665118 Train f1: 0.558023\n",
      " Valid loss: 0.705229 Valid acc: 0.423758 Valid f1: 0.040874\n",
      "Iteration: 25/100\n",
      " Train loss: 0.652389 Train acc: 0.664673 Train f1: 0.559429\n",
      " Valid loss: 0.708337 Valid acc: 0.411535 Valid f1: 0.040487\n",
      "Iteration: 26/100\n",
      " Train loss: 0.650262 Train acc: 0.664434 Train f1: 0.561452\n",
      " Valid loss: 0.711933 Valid acc: 0.398326 Valid f1: 0.040136\n",
      "Iteration: 27/100\n",
      " Train loss: 0.648128 Train acc: 0.663975 Train f1: 0.562889\n",
      " Valid loss: 0.715774 Valid acc: 0.386732 Valid f1: 0.040065\n",
      "Iteration: 28/100\n",
      " Train loss: 0.645997 Train acc: 0.663709 Train f1: 0.563758\n",
      " Valid loss: 0.719616 Valid acc: 0.377160 Valid f1: 0.040526\n",
      "Iteration: 29/100\n",
      " Train loss: 0.643862 Train acc: 0.664270 Train f1: 0.565188\n",
      " Valid loss: 0.723284 Valid acc: 0.369285 Valid f1: 0.041078\n",
      "Iteration: 30/100\n",
      " Train loss: 0.641720 Train acc: 0.664988 Train f1: 0.566122\n",
      " Valid loss: 0.726772 Valid acc: 0.364089 Valid f1: 0.041151\n",
      "Iteration: 31/100\n",
      " Train loss: 0.639579 Train acc: 0.666076 Train f1: 0.567082\n",
      " Valid loss: 0.730161 Valid acc: 0.359057 Valid f1: 0.041077\n",
      "Iteration: 32/100\n",
      " Train loss: 0.637445 Train acc: 0.667355 Train f1: 0.568109\n",
      " Valid loss: 0.733585 Valid acc: 0.354244 Valid f1: 0.041017\n",
      "Iteration: 33/100\n",
      " Train loss: 0.635326 Train acc: 0.668395 Train f1: 0.568908\n",
      " Valid loss: 0.737276 Valid acc: 0.349787 Valid f1: 0.041057\n",
      "Iteration: 34/100\n",
      " Train loss: 0.633228 Train acc: 0.669203 Train f1: 0.569597\n",
      " Valid loss: 0.741520 Valid acc: 0.344153 Valid f1: 0.040949\n",
      "Iteration: 35/100\n",
      " Train loss: 0.631153 Train acc: 0.670010 Train f1: 0.570984\n",
      " Valid loss: 0.746580 Valid acc: 0.336496 Valid f1: 0.040950\n",
      "Iteration: 36/100\n",
      " Train loss: 0.629105 Train acc: 0.670605 Train f1: 0.572719\n",
      " Valid loss: 0.752621 Valid acc: 0.327472 Valid f1: 0.040947\n",
      "Iteration: 37/100\n",
      " Train loss: 0.627083 Train acc: 0.672022 Train f1: 0.576589\n",
      " Valid loss: 0.759655 Valid acc: 0.316479 Valid f1: 0.040683\n",
      "Iteration: 38/100\n",
      " Train loss: 0.625085 Train acc: 0.672973 Train f1: 0.580076\n",
      " Valid loss: 0.767482 Valid acc: 0.304310 Valid f1: 0.040145\n",
      "Iteration: 39/100\n",
      " Train loss: 0.623114 Train acc: 0.673116 Train f1: 0.582144\n",
      " Valid loss: 0.775788 Valid acc: 0.290883 Valid f1: 0.039486\n",
      "Iteration: 40/100\n",
      " Train loss: 0.621168 Train acc: 0.674512 Train f1: 0.585805\n",
      " Valid loss: 0.784207 Valid acc: 0.278276 Valid f1: 0.039033\n",
      "Iteration: 41/100\n",
      " Train loss: 0.619238 Train acc: 0.676168 Train f1: 0.589292\n",
      " Valid loss: 0.792501 Valid acc: 0.268404 Valid f1: 0.038872\n",
      "Iteration: 42/100\n",
      " Train loss: 0.617332 Train acc: 0.677701 Train f1: 0.592191\n",
      " Valid loss: 0.800617 Valid acc: 0.259407 Valid f1: 0.038691\n",
      "Iteration: 43/100\n",
      " Train loss: 0.615419 Train acc: 0.680656 Train f1: 0.597068\n",
      " Valid loss: 0.808676 Valid acc: 0.250520 Valid f1: 0.038452\n",
      "Iteration: 44/100\n",
      " Train loss: 0.613517 Train acc: 0.683523 Train f1: 0.601714\n",
      " Valid loss: 0.816883 Valid acc: 0.242234 Valid f1: 0.038181\n",
      "Iteration: 45/100\n",
      " Train loss: 0.611600 Train acc: 0.686520 Train f1: 0.606603\n",
      " Valid loss: 0.825493 Valid acc: 0.233237 Valid f1: 0.037882\n",
      "Iteration: 46/100\n",
      " Train loss: 0.609696 Train acc: 0.689599 Train f1: 0.611913\n",
      " Valid loss: 0.834653 Valid acc: 0.222790 Valid f1: 0.037522\n",
      "Iteration: 47/100\n",
      " Train loss: 0.607777 Train acc: 0.692589 Train f1: 0.616906\n",
      " Valid loss: 0.844256 Valid acc: 0.211770 Valid f1: 0.037275\n",
      "Iteration: 48/100\n",
      " Train loss: 0.605817 Train acc: 0.695695 Train f1: 0.622323\n",
      " Valid loss: 0.853969 Valid acc: 0.201159 Valid f1: 0.036861\n",
      "Iteration: 49/100\n",
      " Train loss: 0.603815 Train acc: 0.699438 Train f1: 0.628341\n",
      " Valid loss: 0.863378 Valid acc: 0.191944 Valid f1: 0.036582\n",
      "Iteration: 50/100\n",
      " Train loss: 0.601789 Train acc: 0.703283 Train f1: 0.634123\n",
      " Valid loss: 0.872189 Valid acc: 0.185545 Valid f1: 0.036305\n",
      "Iteration: 51/100\n",
      " Train loss: 0.599748 Train acc: 0.707149 Train f1: 0.639477\n",
      " Valid loss: 0.880146 Valid acc: 0.179447 Valid f1: 0.036107\n",
      "Iteration: 52/100\n",
      " Train loss: 0.597716 Train acc: 0.711007 Train f1: 0.644108\n",
      " Valid loss: 0.887282 Valid acc: 0.175290 Valid f1: 0.036054\n",
      "Iteration: 53/100\n",
      " Train loss: 0.595669 Train acc: 0.714285 Train f1: 0.647921\n",
      " Valid loss: 0.894280 Valid acc: 0.171489 Valid f1: 0.035956\n",
      "Iteration: 54/100\n",
      " Train loss: 0.593596 Train acc: 0.717637 Train f1: 0.651970\n",
      " Valid loss: 0.901869 Valid acc: 0.167688 Valid f1: 0.035859\n",
      "Iteration: 55/100\n",
      " Train loss: 0.591503 Train acc: 0.720785 Train f1: 0.656510\n",
      " Valid loss: 0.910326 Valid acc: 0.162984 Valid f1: 0.035665\n",
      "Iteration: 56/100\n",
      " Train loss: 0.589377 Train acc: 0.724342 Train f1: 0.662025\n",
      " Valid loss: 0.919288 Valid acc: 0.157187 Valid f1: 0.035428\n",
      "Iteration: 57/100\n",
      " Train loss: 0.587226 Train acc: 0.729146 Train f1: 0.669317\n",
      " Valid loss: 0.927929 Valid acc: 0.152018 Valid f1: 0.035220\n",
      "Iteration: 58/100\n",
      " Train loss: 0.585042 Train acc: 0.733682 Train f1: 0.675860\n",
      " Valid loss: 0.935542 Valid acc: 0.147041 Valid f1: 0.035141\n",
      "Iteration: 59/100\n",
      " Train loss: 0.582828 Train acc: 0.737671 Train f1: 0.681031\n",
      " Valid loss: 0.942098 Valid acc: 0.143842 Valid f1: 0.035014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 60/100\n",
      " Train loss: 0.580591 Train acc: 0.740633 Train f1: 0.684731\n",
      " Valid loss: 0.948224 Valid acc: 0.141490 Valid f1: 0.035100\n",
      "Iteration: 61/100\n",
      " Train loss: 0.578335 Train acc: 0.744513 Train f1: 0.689846\n",
      " Valid loss: 0.954696 Valid acc: 0.138783 Valid f1: 0.034993\n",
      "Iteration: 62/100\n",
      " Train loss: 0.576069 Train acc: 0.748255 Train f1: 0.695394\n",
      " Valid loss: 0.961816 Valid acc: 0.135419 Valid f1: 0.034921\n",
      "Iteration: 63/100\n",
      " Train loss: 0.573791 Train acc: 0.752265 Train f1: 0.701564\n",
      " Valid loss: 0.969065 Valid acc: 0.132301 Valid f1: 0.034800\n",
      "Iteration: 64/100\n",
      " Train loss: 0.571504 Train acc: 0.755720 Train f1: 0.706624\n",
      " Valid loss: 0.975522 Valid acc: 0.129457 Valid f1: 0.034748\n",
      "Iteration: 65/100\n",
      " Train loss: 0.569217 Train acc: 0.758813 Train f1: 0.710808\n",
      " Valid loss: 0.980689 Valid acc: 0.127762 Valid f1: 0.034683\n",
      "Iteration: 66/100\n",
      " Train loss: 0.566934 Train acc: 0.761337 Train f1: 0.713890\n",
      " Valid loss: 0.985125 Valid acc: 0.126668 Valid f1: 0.034641\n",
      "Iteration: 67/100\n",
      " Train loss: 0.564660 Train acc: 0.763506 Train f1: 0.716845\n",
      " Valid loss: 0.989820 Valid acc: 0.125082 Valid f1: 0.034581\n",
      "Iteration: 68/100\n",
      " Train loss: 0.562395 Train acc: 0.766188 Train f1: 0.720975\n",
      " Valid loss: 0.995123 Valid acc: 0.123359 Valid f1: 0.034573\n",
      "Iteration: 69/100\n",
      " Train loss: 0.560143 Train acc: 0.768296 Train f1: 0.724170\n",
      " Valid loss: 1.000340 Valid acc: 0.121992 Valid f1: 0.034521\n",
      "Iteration: 70/100\n",
      " Train loss: 0.557916 Train acc: 0.769992 Train f1: 0.726555\n",
      " Valid loss: 1.004523 Valid acc: 0.121117 Valid f1: 0.034488\n",
      "Iteration: 71/100\n",
      " Train loss: 0.555723 Train acc: 0.771491 Train f1: 0.728595\n",
      " Valid loss: 1.007669 Valid acc: 0.121965 Valid f1: 0.034520\n",
      "Iteration: 72/100\n",
      " Train loss: 0.553569 Train acc: 0.772887 Train f1: 0.730699\n",
      " Valid loss: 1.010752 Valid acc: 0.122101 Valid f1: 0.034525\n",
      "Iteration: 73/100\n",
      " Train loss: 0.551450 Train acc: 0.774501 Train f1: 0.733289\n",
      " Valid loss: 1.014518 Valid acc: 0.121664 Valid f1: 0.034509\n",
      "Iteration: 74/100\n",
      " Train loss: 0.549368 Train acc: 0.776260 Train f1: 0.736284\n",
      " Valid loss: 1.018569 Valid acc: 0.121199 Valid f1: 0.034491\n",
      "Iteration: 75/100\n",
      " Train loss: 0.547328 Train acc: 0.778559 Train f1: 0.739963\n",
      " Valid loss: 1.021966 Valid acc: 0.120953 Valid f1: 0.034482\n",
      "Iteration: 76/100\n",
      " Train loss: 0.545326 Train acc: 0.780269 Train f1: 0.742522\n",
      " Valid loss: 1.024510 Valid acc: 0.122211 Valid f1: 0.034529\n",
      "Iteration: 77/100\n",
      " Train loss: 0.543365 Train acc: 0.782630 Train f1: 0.746104\n",
      " Valid loss: 1.026933 Valid acc: 0.123633 Valid f1: 0.034584\n",
      "Iteration: 78/100\n",
      " Train loss: 0.541436 Train acc: 0.784319 Train f1: 0.748791\n",
      " Valid loss: 1.029891 Valid acc: 0.124207 Valid f1: 0.034605\n",
      "Iteration: 79/100\n",
      " Train loss: 0.539541 Train acc: 0.785914 Train f1: 0.751300\n",
      " Valid loss: 1.033011 Valid acc: 0.124316 Valid f1: 0.034610\n",
      "Iteration: 80/100\n",
      " Train loss: 0.537674 Train acc: 0.787234 Train f1: 0.753377\n",
      " Valid loss: 1.035719 Valid acc: 0.125957 Valid f1: 0.034672\n",
      "Iteration: 81/100\n",
      " Train loss: 0.535836 Train acc: 0.789027 Train f1: 0.756108\n",
      " Valid loss: 1.037826 Valid acc: 0.127625 Valid f1: 0.034736\n",
      "Iteration: 82/100\n",
      " Train loss: 0.534025 Train acc: 0.790484 Train f1: 0.758361\n",
      " Valid loss: 1.039821 Valid acc: 0.128746 Valid f1: 0.034779\n",
      "Iteration: 83/100\n",
      " Train loss: 0.532239 Train acc: 0.791545 Train f1: 0.760093\n",
      " Valid loss: 1.042080 Valid acc: 0.129895 Valid f1: 0.034824\n",
      "Iteration: 84/100\n",
      " Train loss: 0.530481 Train acc: 0.792708 Train f1: 0.761949\n",
      " Valid loss: 1.044472 Valid acc: 0.130360 Valid f1: 0.034842\n",
      "Iteration: 85/100\n",
      " Train loss: 0.528746 Train acc: 0.793584 Train f1: 0.763330\n",
      " Valid loss: 1.046521 Valid acc: 0.131673 Valid f1: 0.034893\n",
      "Iteration: 86/100\n",
      " Train loss: 0.527033 Train acc: 0.794740 Train f1: 0.765019\n",
      " Valid loss: 1.048149 Valid acc: 0.133177 Valid f1: 0.034951\n",
      "Iteration: 87/100\n",
      " Train loss: 0.525341 Train acc: 0.796088 Train f1: 0.766900\n",
      " Valid loss: 1.049732 Valid acc: 0.134790 Valid f1: 0.035014\n",
      "Iteration: 88/100\n",
      " Train loss: 0.523674 Train acc: 0.797285 Train f1: 0.768658\n",
      " Valid loss: 1.051547 Valid acc: 0.136595 Valid f1: 0.035085\n",
      "Iteration: 89/100\n",
      " Train loss: 0.522035 Train acc: 0.798373 Train f1: 0.770145\n",
      " Valid loss: 1.053375 Valid acc: 0.137361 Valid f1: 0.035115\n",
      "Iteration: 90/100\n",
      " Train loss: 0.520416 Train acc: 0.799468 Train f1: 0.771717\n",
      " Valid loss: 1.054863 Valid acc: 0.138974 Valid f1: 0.035178\n",
      "Iteration: 91/100\n",
      " Train loss: 0.518817 Train acc: 0.800597 Train f1: 0.773262\n",
      " Valid loss: 1.055996 Valid acc: 0.140451 Valid f1: 0.035236\n",
      "Iteration: 92/100\n",
      " Train loss: 0.517238 Train acc: 0.801814 Train f1: 0.774951\n",
      " Valid loss: 1.057161 Valid acc: 0.141791 Valid f1: 0.035289\n",
      "Iteration: 93/100\n",
      " Train loss: 0.515680 Train acc: 0.803197 Train f1: 0.776871\n",
      " Valid loss: 1.058540 Valid acc: 0.142584 Valid f1: 0.035321\n",
      "Iteration: 94/100\n",
      " Train loss: 0.514143 Train acc: 0.804599 Train f1: 0.778837\n",
      " Valid loss: 1.059880 Valid acc: 0.143486 Valid f1: 0.035357\n",
      "Iteration: 95/100\n",
      " Train loss: 0.512626 Train acc: 0.806275 Train f1: 0.781062\n",
      " Valid loss: 1.060905 Valid acc: 0.144990 Valid f1: 0.035417\n",
      "Iteration: 96/100\n",
      " Train loss: 0.511123 Train acc: 0.807658 Train f1: 0.782885\n",
      " Valid loss: 1.061800 Valid acc: 0.146440 Valid f1: 0.035475\n",
      "Iteration: 97/100\n",
      " Train loss: 0.509635 Train acc: 0.809252 Train f1: 0.784972\n",
      " Valid loss: 1.062850 Valid acc: 0.147205 Valid f1: 0.035506\n",
      "Iteration: 98/100\n",
      " Train loss: 0.508161 Train acc: 0.811414 Train f1: 0.787812\n",
      " Valid loss: 1.064073 Valid acc: 0.147615 Valid f1: 0.035522\n",
      "Iteration: 99/100\n",
      " Train loss: 0.506699 Train acc: 0.813220 Train f1: 0.790174\n",
      " Valid loss: 1.065173 Valid acc: 0.148135 Valid f1: 0.035543\n",
      "Iteration: 100/100\n",
      " Train loss: 0.505250 Train acc: 0.814780 Train f1: 0.792185\n",
      " Valid loss: 1.065997 Valid acc: 0.149010 Valid f1: 0.035578\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4bd5e4266bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrue_iteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"cnn_2017/2017_save_net.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Save to path:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "vld_acc = []\n",
    "vld_f1 = []\n",
    "vld_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_f1 = []\n",
    "train_loss = []\n",
    "\n",
    "X_train_oversampled = np.array(X_train_oversampled, dtype=np.float32)\n",
    "y_train_oversampled = np.array(y_train_oversampled, dtype=np.float32)\n",
    "X_vld = np.array(X_vld, dtype=np.float32)\n",
    "y_vld = np.array(y_vld, dtype=np.float32)\n",
    "\n",
    "saver = tf.train.Saver(var_dict)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "    true_iteration = 1\n",
    "    for i in range(max_iterations):\n",
    "        feed = {xs : X_train_oversampled, ys : y_train_oversampled, keep_prob : 1, learning_rate : lr}\n",
    "        \n",
    "        # train\n",
    "        loss, _ = sess.run([cost, train_op],feed_dict=feed)\n",
    "        precision, recall, f1_score, accuracy = evaluate(X_train_oversampled, y_train_oversampled, sess)\n",
    "        \n",
    "        train_loss.append(loss)\n",
    "        train_f1.append(f1_score)\n",
    "        train_acc.append(accuracy)\n",
    "        \n",
    "        # vld cost\n",
    "        loss_v = sess.run(cost, feed_dict={xs: X_vld, ys: y_vld, keep_prob: 1})\n",
    "        vld_loss.append(loss_v)\n",
    "        # vld evaluation\n",
    "        vld_precision, vld_recall, vld_f1_score, vld_accuracy = evaluate(X_vld, y_vld, sess)\n",
    "        vld_f1.append(vld_f1_score)\n",
    "        vld_acc.append(vld_accuracy)\n",
    "\n",
    "        if vld_f1_score >= 0.98:\n",
    "            break\n",
    "        print(\"Iteration: {}/{}\\n\".format(true_iteration, max_iterations),\n",
    "              \"Train loss: {:6f}\".format(loss),\n",
    "              \"Train acc: {:.6f}\".format(accuracy),\n",
    "              \"Train f1: {:.6f}\\n\".format(f1_score),\n",
    "              \"Valid loss: {:6f}\".format(loss_v),\n",
    "              \"Valid acc: {:.6f}\".format(vld_accuracy),\n",
    "              \"Valid f1: {:.6f}\".format(vld_f1_score))\n",
    "        true_iteration += 1\n",
    "            \n",
    "    save_path = saver.save(sess,\"cnn_2017/2017_save_net.ckpt\")\n",
    "    print(\"Save to path:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and valid loss\n",
    "t = np.arange(true_iteration - 1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t, np.array(vld_loss), 'b-')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t, vld_acc, 'b-')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_f1), 'r-', t, vld_f1, 'b-')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.legend(['train', 'valid'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
