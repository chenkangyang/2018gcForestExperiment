{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入数据\n",
    "基于 waterData_Training.txt，读入为pandas的DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('../data/water/txt/2017waterDataTraining.txt',delim_whitespace=True)\n",
    "# test = pd.read_table('../data/water/waterData_Testing.txt',delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "Time = np.zeros(df.shape[0]).astype(\"str\")\n",
    "for i in range(len(df)):\n",
    "    Time[i] = df['index'][i]+\" \"+ df['Time'][i]\n",
    "df['Time'] = Time\n",
    "df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Tp</th>\n",
       "      <th>Cl</th>\n",
       "      <th>pH</th>\n",
       "      <th>Redox</th>\n",
       "      <th>Leit</th>\n",
       "      <th>Trueb</th>\n",
       "      <th>Cl_2</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Fm_2</th>\n",
       "      <th>EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-02-15 19:54:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.110</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02-15 19:55:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-02-15 19:56:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1471.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-02-15 19:57:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.37</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-02-15 19:58:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time   Tp    Cl    pH  Redox   Leit  Trueb   Cl_2      Fm  \\\n",
       "0  2016-02-15 19:54:00  4.4  0.14  8.38  755.0  232.0  0.009  0.110  1428.0   \n",
       "1  2016-02-15 19:55:00  4.4  0.14  8.38  755.0  232.0  0.009  0.111  1436.0   \n",
       "2  2016-02-15 19:56:00  4.4  0.14  8.38  755.0  232.0  0.014  0.113  1471.0   \n",
       "3  2016-02-15 19:57:00  4.4  0.14  8.37  755.0  232.0  0.015  0.111  1457.0   \n",
       "4  2016-02-15 19:58:00  4.4  0.14  8.38  755.0  232.0  0.013  0.111  1476.0   \n",
       "\n",
       "     Fm_2  EVENT  \n",
       "0  1020.0  False  \n",
       "1  1018.0  False  \n",
       "2  1019.0  False  \n",
       "3  1015.0  False  \n",
       "4  1019.0  False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Df_shape: 122334 \n",
      "cols: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Df_shape:\",df.shape[0], \"\\ncols:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types:\n",
      " Time      object\n",
      "Tp       float64\n",
      "Cl       float64\n",
      "pH       float64\n",
      "Redox    float64\n",
      "Leit     float64\n",
      "Trueb    float64\n",
      "Cl_2     float64\n",
      "Fm       float64\n",
      "Fm_2     float64\n",
      "EVENT       bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/water/train.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程\n",
    "\n",
    "\n",
    "It looks like we have 14 columns to help us predict our classification. We will drop fnlwgt and education and then convert our categorical features to dummy variables. We will also convert our label to 0 and 1 where 1 means the person made more than $50k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Time']\n",
    "continuous_features = ['Tp', 'Cl', 'pH', 'Redox', 'Leit', 'Trueb', 'Cl_2', 'Fm', 'Fm_2']\n",
    "cat_features =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_dummies = pd.get_dummies(df, columns=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_dummies.drop(drop_columns, 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除空数据\n",
    "all_df_dummies = all_df_dummies.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_df_dummies.drop(['EVENT'], axis=1) # Series\n",
    "y = all_df_dummies['EVENT'].apply(lambda x: 0 if x == False else 1) # Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([X,y], axis=1)\n",
    "# 将一维数组ndarray，a转化为二维数组\n",
    "# y = y[:,np.newaxis];\n",
    "# data = np.concatenate((X,y),axis=1) # ndarray\n",
    "# print(\"data.shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tp</th>\n",
       "      <th>Cl</th>\n",
       "      <th>pH</th>\n",
       "      <th>Redox</th>\n",
       "      <th>Leit</th>\n",
       "      <th>Trueb</th>\n",
       "      <th>Cl_2</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Fm_2</th>\n",
       "      <th>EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.110</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1471.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.37</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tp    Cl    pH  Redox   Leit  Trueb   Cl_2      Fm    Fm_2  EVENT\n",
       "0  4.4  0.14  8.38  755.0  232.0  0.009  0.110  1428.0  1020.0      0\n",
       "1  4.4  0.14  8.38  755.0  232.0  0.009  0.111  1436.0  1018.0      0\n",
       "2  4.4  0.14  8.38  755.0  232.0  0.014  0.113  1471.0  1019.0      0\n",
       "3  4.4  0.14  8.37  755.0  232.0  0.015  0.111  1457.0  1015.0      0\n",
       "4  4.4  0.14  8.38  755.0  232.0  0.013  0.111  1476.0  1019.0      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分层以保证data.label中 0 和 1 均匀分布, data为DataFrame类型\n",
    "def Layering(df, N):\n",
    "    new_data=df.iloc[:,0:]\n",
    "    #实现数据分组\n",
    "    data_maj = new_data[new_data['EVENT']==0]#（30951，21）\n",
    "    data_min = new_data[new_data['EVENT']==1]# (177,21)   #对比很明显，数据量悬殊太大，必须采样！！！\n",
    "    n_maj=data_maj.iloc[:,0].size# maj 数据的行数，方便以后数据量时的自动计算\n",
    "    n_min=data_min.iloc[:,0].size# min 数据行数\n",
    "    M1=n_maj%N    # 大部分数据分成N 份后的 余数\n",
    "    M2=n_min%N    # 小部分数据分成N 份后的 余数\n",
    "    stepD=int(n_maj/10) # 大部分数据  分割步长\n",
    "    stepS=int(n_min/10) # 小部分数据  分割步长\n",
    "    #对 多的部分 的数据分 N 份\n",
    "    maj_data = []\n",
    "    for i in range(N):\n",
    "        maj_data.append(data_maj.iloc[i*stepD:(i+1)*stepD])\n",
    "    for i in range(M1):\n",
    "        maj_data[i]=maj_data[i].append(data_maj.iloc[stepD*N+i:stepD*N+i+1])\n",
    "\n",
    "    #对 少的部分 的数据分 N 份\n",
    "    min_data = []\n",
    "    for i in range(N):\n",
    "        min_data.append(data_min.iloc[i*stepS:(i+1)*stepS])\n",
    "    for i in range(M2):\n",
    "        min_data[i]=min_data[i].append(data_min.iloc[stepS*N+i:stepS*N+i+1])\n",
    "    #重新组合成均匀数据\n",
    "    Last_Data = pd.DataFrame()\n",
    "    for i in range(N):\n",
    "        Last_Data=Last_Data.append(maj_data[i].append(min_data[i]))\n",
    "    return Last_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 分层采样 ============\n",
      "分层采样前维度: (110812, 10)\n",
      "============ 分层采样检验 ============\n",
      "分100层采样后，维度保持一致:110812\n",
      "** 数据均匀分布 **\n",
      "all_data 分布: 0---0.9843%， 1---0.0157%\n",
      "all_data[0:49999] 分布: 0---0.9861%， 1---0.0139%\n",
      "all_data[50000:99999] 分布: 0---0.9826%， 1---0.0174%\n"
     ]
    }
   ],
   "source": [
    "kf=100\n",
    "test_size =  0.33\n",
    "print(\"============ 分层采样 ============\")\n",
    "print(\"分层采样前维度:\", data_all.shape)\n",
    "data_layer = Layering(data_all, kf)\n",
    "print(\"============ 分层采样检验 ============\")\n",
    "print(\"分%d层采样后，维度保持一致:%d\" %(kf, data_layer.shape[0]))\n",
    "print(\"** 数据均匀分布 **\")\n",
    "print(\"all_data 分布: 0---%.4f%%， 1---%.4f%%\" % (data_layer[\"EVENT\"].value_counts(normalize=True)[0], data_layer[\"EVENT\"].value_counts(normalize=True)[1]))\n",
    "print(\"all_data[0:49999] 分布: 0---%.4f%%， 1---%.4f%%\" % (data_layer.iloc[0:49999][\"EVENT\"].value_counts(normalize=True)[0], data_layer.iloc[0:49999][\"EVENT\"].value_counts(normalize=True)[1]))\n",
    "print(\"all_data[50000:99999] 分布: 0---%.4f%%， 1---%.4f%%\" % (data_layer.iloc[50000:99999][\"EVENT\"].value_counts(normalize=True)[0], data_layer.iloc[50000:99999][\"EVENT\"].value_counts(normalize=True)[1]))\n",
    "array = data_all.values\n",
    "X = array[:, 0:-1] # ndarray\n",
    "y = array[:, -1] # ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解决数据不平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smoter(X, y, is_random=False):\n",
    "    # 是否打乱数据\n",
    "    if is_random == True:\n",
    "        random_lst = list(np.random.randint(0, 1000, 4))\n",
    "    elif is_random == False:\n",
    "        random_lst = [0] * 4\n",
    "\n",
    "    print(\"SMOTE...\")\n",
    "    sm = SMOTE(random_state=random_lst[2])\n",
    "    X_smote, y_smote = sm.fit_sample(X, y)\n",
    "    \n",
    "    print('特征数量', X_smote.shape[1])\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ 划分训练集和验证集 ============\n",
      "训练集: 74244, 验证集: 36568\n"
     ]
    }
   ],
   "source": [
    "print(\"============ 划分训练集和验证集 ============\")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "print(\"训练集: %d, 验证集: %d\" %(X_train.shape[0], X_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上采样之前要做数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 中位数填充缺失值后，将数据标准化, 实际上缺失值全删了\n",
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = clean_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_clean = clean_pipeline.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE...\n",
      "特征数量 9\n",
      "============ SMOTE采样 ============\n",
      "训练集: 74244, 0占0.9843, SMOTE采样: 146158  0占0.5000\n"
     ]
    }
   ],
   "source": [
    "X_train_oversampled, y_train_oversampled = Smoter(X_train_clean, y_train, is_random=True)\n",
    "#     X_smote, y_smote = Smoter(X_train, y_train, is_random=True)\n",
    "print(\"============ SMOTE采样 ============\")\n",
    "print(\"训练集: %d, 0占%.4f, SMOTE采样: %d  0占%.4f\" %(X_train.shape[0], (y_train == 0).sum()/y_train.shape[0], X_train_oversampled.shape[0], (y_train_oversampled == 0).sum()/y_train_oversampled.shape[0]))\n",
    "#     print(\"训练集数据SMOTE\", X_smote.shape)  # (43352, 21)\n",
    "\n",
    "# forest = RandomForestClassifier(n_estimators=180, min_samples_leaf=5, max_depth=30, min_samples_split=50, max_features=1, criterion='gini')  \n",
    "# forest.fit(X_oversampled, y_oversampled)\n",
    "# # 计算评估集得分\n",
    "# y_pred = forest.predict(X_valid)\n",
    "# print('预测值的1的位置：', np.where(y_pred==1)[0])\n",
    "# print('真实值的1的位置', np.where(y_test==1)[0])\n",
    "# print('f1_score', f1_score(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true, pred):\n",
    "    f1 = metrics.f1_score(true, pred)\n",
    "    roc_auc = metrics.roc_auc_score(true, pred)\n",
    "    accuracy = metrics.accuracy_score(true, pred)\n",
    "    print(\"F1: {0}\\nROC_AUC: {1}\\nACCURACY: {2}\".format(f1, roc_auc, accuracy))\n",
    "    return f1, roc_auc, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The first model up is a simple logistic regression with the default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在分布不均匀的数据上训练逻辑回归模型\n",
      "F1: 0.368271954674221\n",
      "ROC_AUC: 0.6130295866708382\n",
      "ACCURACY: 0.9878035440822577\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_clean, y_train)\n",
    "lr_predictions = clf.predict(X_valid_clean)\n",
    "print(\"在分布不均匀的数据上训练逻辑回归模型\")\n",
    "lr_f1, lr_roc_auc, lr_acc = evaluate(y_valid, lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在SMOTE采样的数据上训练逻辑回归模型\n",
      "F1: 0.10566315273351312\n",
      "ROC_AUC: 0.7754299567911151\n",
      "ACCURACY: 0.8004812951214176\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_oversampled, y_train_oversampled)\n",
    "lr_predictions = clf.predict(X_valid_clean)\n",
    "print(\"在SMOTE采样的数据上训练逻辑回归模型\")\n",
    "lr_f1, lr_roc_auc, lr_acc = evaluate(y_valid, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GcForest\n",
    "\n",
    "The second model up is a gcforest with our hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you wish to use Cascade Layer only, the legal data type for X_train, X_test can be:\n",
    "\n",
    "    2-D numpy array of shape (n_sampels, n_features).\n",
    "    3-D or 4-D numpy array are also acceptable. For example, passing X_train of shape (60000, 28, 28) or (60000,3,28,28) will be automatically be reshape into (60000, 784)/(60000,2352).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.utils.config_utils import load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", dest=\"model\", type=str, default=None, help=\"gcfoest Net Model File\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_toy_config():\n",
    "    config = {}\n",
    "    ca_config = {}\n",
    "    ca_config[\"random_state\"] = 0\n",
    "    ca_config[\"max_layers\"] = 10\n",
    "    ca_config[\"early_stopping_rounds\"] = 3\n",
    "    ca_config[\"n_classes\"] = 2\n",
    "    ca_config[\"estimators\"] = []\n",
    "#     ca_config[\"estimators\"].append(\n",
    "#             {\"n_folds\": 5, \"type\": \"XGBClassifier\", \"n_estimators\": 10, \"max_depth\": 5,\n",
    "#              \"objective\": \"multi:softprob\", \"silent\": True, \"nthread\": -1, \"learning_rate\": 0.1} )\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"RandomForestClassifier\", \"n_estimators\": 10, \"max_depth\": None, \"n_jobs\": -1})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"ExtraTreesClassifier\", \"n_estimators\": 10, \"max_depth\": None, \"n_jobs\": -1})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"LogisticRegression\"})\n",
    "    config[\"cascade\"] = ca_config\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-10-15 17:55:57,658][cascade_classifier.fit_transform] X_groups_train.shape=[(146158, 9)],y_train.shape=(146158,),X_groups_test.shape=no_test,y_test.shape=no_test\n",
      "[ 2018-10-15 17:55:57,665][cascade_classifier.fit_transform] group_dims=[9]\n",
      "[ 2018-10-15 17:55:57,666][cascade_classifier.fit_transform] group_starts=[0]\n",
      "[ 2018-10-15 17:55:57,667][cascade_classifier.fit_transform] group_ends=[9]\n",
      "[ 2018-10-15 17:55:57,668][cascade_classifier.fit_transform] X_train.shape=(146158, 9),X_test.shape=(0, 9)\n",
      "[ 2018-10-15 17:55:57,680][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(146158, 9), X_cur_test.shape=(0, 9)\n",
      "[ 2018-10-15 17:55:58,935][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_0.predict)=99.99%\n",
      "[ 2018-10-15 17:55:59,886][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_1.predict)=99.99%\n",
      "[ 2018-10-15 17:56:00,946][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_2.predict)=99.98%\n",
      "[ 2018-10-15 17:56:01,789][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_3.predict)=99.99%\n",
      "[ 2018-10-15 17:56:02,745][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:02,748][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 5_folds.train_cv.predict)=99.99%\n",
      "[ 2018-10-15 17:56:03,260][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_0.predict)=100.00%\n",
      "[ 2018-10-15 17:56:03,700][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_1.predict)=100.00%\n",
      "[ 2018-10-15 17:56:04,252][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:04,694][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_3.predict)=99.99%\n",
      "[ 2018-10-15 17:56:05,125][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_4.predict)=100.00%\n",
      "[ 2018-10-15 17:56:05,131][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:05,443][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_0.predict)=78.72%\n",
      "[ 2018-10-15 17:56:05,750][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_1.predict)=78.39%\n",
      "[ 2018-10-15 17:56:06,111][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_2.predict)=78.57%\n",
      "[ 2018-10-15 17:56:06,370][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_3.predict)=78.65%\n",
      "[ 2018-10-15 17:56:06,602][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_4.predict)=78.42%\n",
      "[ 2018-10-15 17:56:06,606][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 5_folds.train_cv.predict)=78.55%\n",
      "[ 2018-10-15 17:56:06,611][cascade_classifier.calc_accuracy] Accuracy(layer_0 - train.classifier_average)=99.99%\n",
      "[ 2018-10-15 17:56:06,626][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(146158, 15), X_cur_test.shape=(0, 15)\n",
      "[ 2018-10-15 17:56:07,008][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_0.predict)=99.99%\n",
      "[ 2018-10-15 17:56:07,451][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_1.predict)=99.99%\n",
      "[ 2018-10-15 17:56:07,900][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:08,344][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:09,231][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_4.predict)=100.00%\n",
      "[ 2018-10-15 17:56:09,235][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:09,674][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_0.predict)=100.00%\n",
      "[ 2018-10-15 17:56:10,150][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_1.predict)=99.99%\n",
      "[ 2018-10-15 17:56:10,510][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_2.predict)=99.99%\n",
      "[ 2018-10-15 17:56:10,756][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:11,014][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:11,018][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:11,929][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_0.predict)=99.99%\n",
      "[ 2018-10-15 17:56:12,850][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_1.predict)=100.00%\n",
      "[ 2018-10-15 17:56:13,709][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:14,740][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_3.predict)=99.99%\n",
      "[ 2018-10-15 17:56:15,848][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:15,853][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 5_folds.train_cv.predict)=99.99%\n",
      "[ 2018-10-15 17:56:15,936][cascade_classifier.calc_accuracy] Accuracy(layer_1 - train.classifier_average)=100.00%\n",
      "[ 2018-10-15 17:56:15,961][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(146158, 15), X_cur_test.shape=(0, 15)\n",
      "[ 2018-10-15 17:56:16,413][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_0.predict)=100.00%\n",
      "[ 2018-10-15 17:56:16,810][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_1.predict)=99.99%\n",
      "[ 2018-10-15 17:56:17,263][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_2.predict)=99.99%\n",
      "[ 2018-10-15 17:56:17,731][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_3.predict)=99.99%\n",
      "[ 2018-10-15 17:56:18,082][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:18,086][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 5_folds.train_cv.predict)=99.99%\n",
      "[ 2018-10-15 17:56:18,925][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_0.predict)=100.00%\n",
      "[ 2018-10-15 17:56:19,400][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_1.predict)=100.00%\n",
      "[ 2018-10-15 17:56:19,756][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:20,321][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:20,785][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:20,788][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:21,558][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_0.predict)=100.00%\n",
      "[ 2018-10-15 17:56:22,270][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_1.predict)=99.99%\n",
      "[ 2018-10-15 17:56:22,825][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:23,552][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:24,211][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_4.predict)=100.00%\n",
      "[ 2018-10-15 17:56:24,222][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:24,236][cascade_classifier.calc_accuracy] Accuracy(layer_2 - train.classifier_average)=100.00%\n",
      "[ 2018-10-15 17:56:24,254][cascade_classifier.fit_transform] [layer=3] look_indexs=[0], X_cur_train.shape=(146158, 15), X_cur_test.shape=(0, 15)\n",
      "[ 2018-10-15 17:56:24,913][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_0.predict)=99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-10-15 17:56:25,393][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_1.predict)=99.99%\n",
      "[ 2018-10-15 17:56:25,976][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:26,669][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:27,019][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_4.predict)=100.00%\n",
      "[ 2018-10-15 17:56:27,029][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:27,581][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_0.predict)=99.99%\n",
      "[ 2018-10-15 17:56:28,061][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_1.predict)=99.99%\n",
      "[ 2018-10-15 17:56:28,429][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:28,896][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:29,490][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:29,493][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 5_folds.train_cv.predict)=99.99%\n",
      "[ 2018-10-15 17:56:30,154][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_0.predict)=100.00%\n",
      "[ 2018-10-15 17:56:30,849][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_1.predict)=100.00%\n",
      "[ 2018-10-15 17:56:31,596][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:32,169][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:32,679][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:32,683][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:32,691][cascade_classifier.calc_accuracy] Accuracy(layer_3 - train.classifier_average)=100.00%\n",
      "[ 2018-10-15 17:56:32,702][cascade_classifier.fit_transform] [layer=4] look_indexs=[0], X_cur_train.shape=(146158, 15), X_cur_test.shape=(0, 15)\n",
      "[ 2018-10-15 17:56:33,313][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_0.predict)=100.00%\n",
      "[ 2018-10-15 17:56:33,647][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_1.predict)=99.99%\n",
      "[ 2018-10-15 17:56:34,087][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:34,427][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:34,882][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_4.predict)=100.00%\n",
      "[ 2018-10-15 17:56:34,885][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:35,265][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_0.predict)=99.99%\n",
      "[ 2018-10-15 17:56:35,699][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_1.predict)=100.00%\n",
      "[ 2018-10-15 17:56:36,040][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_2.predict)=99.99%\n",
      "[ 2018-10-15 17:56:36,492][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:36,725][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:36,728][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:37,389][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_0.predict)=100.00%\n",
      "[ 2018-10-15 17:56:38,050][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_1.predict)=100.00%\n",
      "[ 2018-10-15 17:56:38,709][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_2.predict)=100.00%\n",
      "[ 2018-10-15 17:56:39,431][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_3.predict)=100.00%\n",
      "[ 2018-10-15 17:56:40,024][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_4.predict)=99.99%\n",
      "[ 2018-10-15 17:56:40,028][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 5_folds.train_cv.predict)=100.00%\n",
      "[ 2018-10-15 17:56:40,034][cascade_classifier.calc_accuracy] Accuracy(layer_4 - train.classifier_average)=100.00%\n",
      "[ 2018-10-15 17:56:40,034][cascade_classifier.fit_transform] [Result][Optimal Level Detected] opt_layer_num=2, accuracy_train=100.00%, accuracy_test=0.00%\n"
     ]
    }
   ],
   "source": [
    "config = get_toy_config()\n",
    "gc = GCForest(config)\n",
    "\n",
    "# If the model you use cost too much memory for you.\n",
    "# You can use these methods to force gcforest not keeping model in memory\n",
    "# gc.set_keep_model_in_mem(False), default is TRUE.\n",
    "\n",
    "X_train_enc = gc.fit_transform(X_train_oversampled, y_train_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-10-15 17:56:40,047][cascade_classifier.transform] X_groups_test.shape=[(36568, 9)]\n",
      "[ 2018-10-15 17:56:40,050][cascade_classifier.transform] group_dims=[9]\n",
      "[ 2018-10-15 17:56:40,051][cascade_classifier.transform] X_test.shape=(36568, 9)\n",
      "[ 2018-10-15 17:56:40,055][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(36568, 9)\n",
      "[ 2018-10-15 17:56:41,303][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(36568, 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9938971229293809\n",
      "ROC_AUC: 0.9956243907329807\n",
      "ACCURACY: 0.9998085758039816\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = gc.predict(X_valid_clean)\n",
    "gc_f1, gc_roc_auc, gc_acc = evaluate(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= results on train =============\n",
      "TP= 570 FP= 2 TN= 35991 FN= 5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc_test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7fe2f95bbf18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TP='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FP='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TN='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FN='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mF1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\nrecall\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\naccuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgc_test_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc_test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "# compute precision and recall\n",
    "TP, FP, TN, FN = 0, 0, 0, 0\n",
    "\n",
    "for i in range(0, len(y_valid_pred)):\n",
    "    if y_valid_pred[i] == y_valid[i] and y_valid[i] == 1:\n",
    "        TP += 1\n",
    "    elif y_valid_pred[i] == y_valid[i] and y_valid[i] == 0:\n",
    "        TN += 1\n",
    "    elif y_valid_pred[i] != y_valid[i] and y_valid[i] == 0:\n",
    "        FP += 1\n",
    "    elif y_valid_pred[i] != y_valid[i] and y_valid[i] == 1:\n",
    "        FN += 1\n",
    "\n",
    "precision = TP/(TP + FP)\n",
    "recall = TP/(TP + FN)\n",
    "print(\"============= 2017 datasets' results on train =============\")\n",
    "print('TP=',TP,'FP=',FP,'TN=',TN,'FN=',FN)\n",
    "F1 = 2*precision*recall / (precision + recall)\n",
    "print(\"precision\",precision,\"\\nrecall\",recall,\"\\naccuracy\",gc_test_acc)\n",
    "print('F1=',F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_test(file_path, start)\n",
    "import numpy as np\n",
    "lines = open(\"../data/water/txt/2017waterDataTesting.txt\").readlines()\n",
    "num_lines = len(lines) - 1\n",
    "\n",
    "X_test = np.ones((num_lines, 9))\n",
    "y_test = np.ones((num_lines, 1))\n",
    "flag = 0\n",
    "\n",
    "lines = np.delete(lines, 0, axis = 0)\n",
    "i = 0\n",
    "\n",
    "for line in lines:\n",
    "    data_line = line.split()\n",
    "    feature = data_line[3:12]\n",
    "    for k in range(9):\n",
    "        if feature[k] == 'NA':\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "        flag = 0\n",
    "        continue    # jump out of the loop\n",
    "    X_test[i] = feature    \n",
    "    if data_line[12] == 'FALSE':\n",
    "        y_test[i] = 0\n",
    "    elif data_line[12] == 'TRUE':\n",
    "        y_test[i] = 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete(X_test, range(i, num_lines), axis = 0)\n",
    "np.delete(y_test, range(i, num_lines), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_clean = clean_pipeline.transform(X_test) \n",
    "y_test_pred = gc.predict(X_test_clean)\n",
    "gc_test_f1, gc_test_roc_auc, gc_test_acc = evaluate(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute precision and recall\n",
    "TP, FP, TN, FN = 0, 0, 0, 0\n",
    "\n",
    "for i in range(0, len(y_test_pred)):\n",
    "    if y_test_pred[i] == y_test[i] and y_test[i] == 1:\n",
    "        TP += 1\n",
    "    elif y_test_pred[i] == y_test[i] and y_test[i] == 0:\n",
    "        TN += 1\n",
    "    elif y_test_pred[i] != y_test[i] and y_test[i] == 0:\n",
    "        FP += 1\n",
    "    elif y_test_pred[i] != y_test[i] and y_test[i] == 1:\n",
    "        FN += 1\n",
    "\n",
    "precision = TP/(TP + FP)\n",
    "recall = TP/(TP + FN)\n",
    "print(\"============= 2017 datasets' results on test =============\")\n",
    "print('TP=',TP,'FP=',FP,'TN=',TN,'FN=',FN)\n",
    "F1 = 2*precision*recall / (precision + recall)\n",
    "print(\"precision\",precision,\"\\nrecall\",recall,\"\\naccuracy\",gc_test_acc)\n",
    "print('F1=',F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
