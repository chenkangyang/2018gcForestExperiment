{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.utils.config_utils import load_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### somte sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smoter(X, y, is_random=False):\n",
    "    if is_random == True:\n",
    "        # random_lst = list(np.random.randint(0, 1000, 4))\n",
    "        sm = SMOTE(random_state=random_seed)\n",
    "    elif is_random == False:\n",
    "        sm = SMOTE(random_state=0)\n",
    "\n",
    "    # sm = SMOTE(random_state=random_lst[2])\n",
    "    X_smote, y_smote = sm.fit_sample(X, y)\n",
    "\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true, pred):\n",
    "    # compute accuracy, precision and recall\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "\n",
    "    for i in range(0, len(pred)):\n",
    "        if pred[i] == true[i] and true[i] == 1:\n",
    "            TP += 1\n",
    "        elif pred[i] == true[i] and true[i] == 0:\n",
    "            TN += 1\n",
    "        elif pred[i] != true[i] and true[i] == 0:\n",
    "            FP += 1\n",
    "        elif pred[i] != true[i] and true[i] == 1:\n",
    "            FN += 1\n",
    "\n",
    "    precision = TP/(TP + FP)\n",
    "    recall = TP/(TP + FN)\n",
    "    accuracy = (TP+TN)/(TP+TN+FN+FP)\n",
    "    \n",
    "    print('TP=',TP,'FP=',FP,'TN=',TN,'FN=',FN)\n",
    "    F1 = 2*precision*recall / (precision + recall)\n",
    "    print(\"precision\", precision,\"\\nrecall\", recall,\"\\naccuracy\", accuracy)\n",
    "    print('F1=',F1)\n",
    "    return F1, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch\n",
    "\n",
    "combine serveral datasâ€˜ features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch(X, y, size):\n",
    "    batch_size = size\n",
    "\n",
    "    X_trim = X\n",
    "    y_trim = y\n",
    "\n",
    "    if len(X) % batch_size != 0:\n",
    "        extra_num = len(X) % batch_size\n",
    "        X_trim = np.delete(X, range(len(X) - extra_num, len(X)), axis = 0)\n",
    "        y_trim = np.delete(y, range(len(y) - extra_num, len(y)), axis = 0)\n",
    "\n",
    "    X_batch = np.split(X_trim, len(X_trim)/batch_size)\n",
    "    y_batch = np.split(y_trim, len(y_trim)/batch_size)\n",
    "\n",
    "    num_batch = 0\n",
    "\n",
    "    for each_batch in X_batch:\n",
    "        X_batch[num_batch] = np.reshape(X_batch[num_batch], (9*batch_size))\n",
    "        y_batch[num_batch] = y_batch[num_batch][-1]\n",
    "        num_batch += 1\n",
    "\n",
    "    X_batch = np.array(X_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gc_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_config():\n",
    "    config = {}\n",
    "    ca_config = {}\n",
    "    ca_config[\"random_state\"] = random_seed\n",
    "    ca_config[\"max_layers\"] = 10\n",
    "    ca_config[\"early_stopping_rounds\"] = 3\n",
    "    ca_config[\"n_classes\"] = 2\n",
    "    ca_config[\"estimators\"] = []\n",
    "#     ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"RandomForestClassifier\", \"n_estimators\": 10, \"max_depth\": None, \"n_jobs\": -1})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \n",
    "                                    \"type\": \"XGBClassifier\", \n",
    "                                    \"n_estimators\": 1000, \n",
    "                                    \"learning_rate\": 0.1,\n",
    "                                    \"gamma\":0,\n",
    "                                    \"subsample\":0.8,\n",
    "                                    \"colsample_bytree\":0.8,\n",
    "                                    \"objective\":'binary:logistic',\n",
    "                                    \"scale_pos_weight\":1,\n",
    "                                    \"seed\":random_seed,\n",
    "                                    \"n_jobs\": -1})\n",
    "\n",
    "#     ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"ExtraTreesClassifier\", \"n_estimators\": 10, \"max_depth\": None, \"n_jobs\": -1})\n",
    "#     ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"LogisticRegression\"})\n",
    "    config[\"cascade\"] = ca_config\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "test_size = 0.33\n",
    "random_seed = 42\n",
    "cv = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_csv(\"../data/water/csv/train2018.csv\")\n",
    "\n",
    "X_train = data_all.values[:, 0:-1]\n",
    "y_train = data_all.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_valid_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"============ train_valid_split ============\")\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y,test_size=test_size, stratify=y, random_state=random_seed)\n",
    "# print(\"train: %d, valid: %d\" %(X_train.shape[0], X_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the data before somte\n",
    "\n",
    "fulfill the Na with median, then standardized the data, output type ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])\n",
    "X_train = clean_pipeline.fit_transform(X_train)\n",
    "# X_valid = clean_pipeline.fit_transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do somte sampling on the train data to solve data imblance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_oversampled, y_train_oversampled = Smoter(X_train, y_train, is_random=True)\n",
    "# print(\"============ SMOTE ============\")\n",
    "# print(\"train: %d, contains %.4f of 0 , after SMOTE: train: %d contains %.4f of 1\" %(X_train.shape[0], (y_train == 0).sum()/y_train.shape[0], X_train_oversampled.shape[0], (y_train_oversampled == 0).sum()/y_train_oversampled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_oversampled_batch, y_train_oversampled_batch = Batch(X_train_oversampled, y_train_oversampled, batch_size)\n",
    "# X_train_batch, y_train_batch = Batch(X_train, y_train, batch_size)\n",
    "# X_valid_batch, y_valid_batch = Batch(X_valid, y_valid, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GcForest\n",
    "\n",
    "## test gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load 2018 Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/water/csv/test2018.csv\")\n",
    "\n",
    "X_test = test.values[:, 0:-1]\n",
    "y_test = test.values[:, -1]\n",
    "\n",
    "X_test = clean_pipeline.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-12-01 16:06:16,857][cascade_classifier.fit_transform] X_groups_train.shape=[(138521, 9)],y_train.shape=(138521,),X_groups_test.shape=[(139566, 9)],y_test.shape=(139566,)\n",
      "[ 2018-12-01 16:06:16,863][cascade_classifier.fit_transform] group_dims=[9]\n",
      "[ 2018-12-01 16:06:16,864][cascade_classifier.fit_transform] group_starts=[0]\n",
      "[ 2018-12-01 16:06:16,865][cascade_classifier.fit_transform] group_ends=[9]\n",
      "[ 2018-12-01 16:06:16,866][cascade_classifier.fit_transform] X_train.shape=(138521, 9),X_test.shape=(139566, 9)\n",
      "[ 2018-12-01 16:06:16,884][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(138521, 9), X_cur_test.shape=(139566, 9)\n",
      "[ 2018-12-01 16:06:46,160][kfold_wrapper.log_eval_metrics] F1 (layer_0 - estimator_0 - 5_folds.train_0.predict)=98.99%\n",
      "[ 2018-12-01 16:07:18,354][kfold_wrapper.log_eval_metrics] F1 (layer_0 - estimator_0 - 5_folds.train_1.predict)=98.83%\n",
      "[ 2018-12-01 16:07:50,513][kfold_wrapper.log_eval_metrics] F1 (layer_0 - estimator_0 - 5_folds.train_2.predict)=97.95%\n",
      "[ 2018-12-01 16:08:22,751][kfold_wrapper.log_eval_metrics] F1 (layer_0 - estimator_0 - 5_folds.train_3.predict)=98.24%\n",
      "[ 2018-12-01 16:08:55,090][kfold_wrapper.log_eval_metrics] F1 (layer_0 - estimator_0 - 5_folds.train_4.predict)=97.65%\n",
      "[ 2018-12-01 16:08:58,198][kfold_wrapper.log_eval_metrics] F1 (layer_0 - estimator_0 - 5_folds.train_cv.predict)=98.33%\n",
      "[ 2018-12-01 16:08:58,231][kfold_wrapper.log_eval_metrics] F1 (layer_0 - estimator_0 - 5_folds.test.predict)=95.61%\n",
      "[ 2018-12-01 16:08:58,278][cascade_classifier.calc_f1] F1 (layer_0 - train.classifier_average)=98.33%\n",
      "[ 2018-12-01 16:08:58,327][cascade_classifier.calc_f1] F1 (layer_0 - test.classifier_average)=95.61%\n",
      "[ 2018-12-01 16:08:58,344][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(138521, 11), X_cur_test.shape=(139566, 11)\n",
      "[ 2018-12-01 16:09:31,004][kfold_wrapper.log_eval_metrics] F1 (layer_1 - estimator_0 - 5_folds.train_0.predict)=97.38%\n",
      "[ 2018-12-01 16:10:07,065][kfold_wrapper.log_eval_metrics] F1 (layer_1 - estimator_0 - 5_folds.train_1.predict)=97.66%\n",
      "[ 2018-12-01 16:10:45,724][kfold_wrapper.log_eval_metrics] F1 (layer_1 - estimator_0 - 5_folds.train_2.predict)=99.27%\n",
      "[ 2018-12-01 16:11:25,786][kfold_wrapper.log_eval_metrics] F1 (layer_1 - estimator_0 - 5_folds.train_3.predict)=98.83%\n",
      "[ 2018-12-01 16:12:05,021][kfold_wrapper.log_eval_metrics] F1 (layer_1 - estimator_0 - 5_folds.train_4.predict)=97.68%\n",
      "[ 2018-12-01 16:12:08,449][kfold_wrapper.log_eval_metrics] F1 (layer_1 - estimator_0 - 5_folds.train_cv.predict)=98.16%\n",
      "[ 2018-12-01 16:12:08,494][kfold_wrapper.log_eval_metrics] F1 (layer_1 - estimator_0 - 5_folds.test.predict)=94.25%\n",
      "[ 2018-12-01 16:12:08,539][cascade_classifier.calc_f1] F1 (layer_1 - train.classifier_average)=98.16%\n",
      "[ 2018-12-01 16:12:08,633][cascade_classifier.calc_f1] F1 (layer_1 - test.classifier_average)=94.25%\n",
      "[ 2018-12-01 16:12:08,727][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(138521, 11), X_cur_test.shape=(139566, 11)\n",
      "[ 2018-12-01 16:12:42,947][kfold_wrapper.log_eval_metrics] F1 (layer_2 - estimator_0 - 5_folds.train_0.predict)=97.67%\n",
      "[ 2018-12-01 16:13:19,909][kfold_wrapper.log_eval_metrics] F1 (layer_2 - estimator_0 - 5_folds.train_1.predict)=98.11%\n",
      "[ 2018-12-01 16:13:56,685][kfold_wrapper.log_eval_metrics] F1 (layer_2 - estimator_0 - 5_folds.train_2.predict)=98.09%\n",
      "[ 2018-12-01 16:14:35,140][kfold_wrapper.log_eval_metrics] F1 (layer_2 - estimator_0 - 5_folds.train_3.predict)=98.26%\n",
      "[ 2018-12-01 16:15:13,941][kfold_wrapper.log_eval_metrics] F1 (layer_2 - estimator_0 - 5_folds.train_4.predict)=97.80%\n",
      "[ 2018-12-01 16:15:17,201][kfold_wrapper.log_eval_metrics] F1 (layer_2 - estimator_0 - 5_folds.train_cv.predict)=97.99%\n",
      "[ 2018-12-01 16:15:17,237][kfold_wrapper.log_eval_metrics] F1 (layer_2 - estimator_0 - 5_folds.test.predict)=90.76%\n",
      "[ 2018-12-01 16:15:17,270][cascade_classifier.calc_f1] F1 (layer_2 - train.classifier_average)=97.99%\n",
      "[ 2018-12-01 16:15:17,302][cascade_classifier.calc_f1] F1 (layer_2 - test.classifier_average)=90.76%\n",
      "[ 2018-12-01 16:15:17,315][cascade_classifier.fit_transform] [layer=3] look_indexs=[0], X_cur_train.shape=(138521, 11), X_cur_test.shape=(139566, 11)\n",
      "[ 2018-12-01 16:15:54,159][kfold_wrapper.log_eval_metrics] F1 (layer_3 - estimator_0 - 5_folds.train_0.predict)=97.66%\n",
      "[ 2018-12-01 16:16:33,958][kfold_wrapper.log_eval_metrics] F1 (layer_3 - estimator_0 - 5_folds.train_1.predict)=97.65%\n",
      "[ 2018-12-01 16:17:15,273][kfold_wrapper.log_eval_metrics] F1 (layer_3 - estimator_0 - 5_folds.train_2.predict)=96.65%\n",
      "[ 2018-12-01 16:17:54,426][kfold_wrapper.log_eval_metrics] F1 (layer_3 - estimator_0 - 5_folds.train_3.predict)=98.40%\n",
      "[ 2018-12-01 16:18:31,440][kfold_wrapper.log_eval_metrics] F1 (layer_3 - estimator_0 - 5_folds.train_4.predict)=98.10%\n",
      "[ 2018-12-01 16:18:34,531][kfold_wrapper.log_eval_metrics] F1 (layer_3 - estimator_0 - 5_folds.train_cv.predict)=97.69%\n",
      "[ 2018-12-01 16:18:34,564][kfold_wrapper.log_eval_metrics] F1 (layer_3 - estimator_0 - 5_folds.test.predict)=89.72%\n",
      "[ 2018-12-01 16:18:34,596][cascade_classifier.calc_f1] F1 (layer_3 - train.classifier_average)=97.69%\n",
      "[ 2018-12-01 16:18:34,630][cascade_classifier.calc_f1] F1 (layer_3 - test.classifier_average)=89.72%\n",
      "[ 2018-12-01 16:18:34,631][cascade_classifier.fit_transform] [Result][Optimal Level Detected] opt_layer_num=1, f1_train=98.33%, f1_test=95.61%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[9.9999982e-01, 1.5955165e-07],\n",
       "        [9.9999911e-01, 8.6565228e-07],\n",
       "        [9.9999982e-01, 2.0300962e-07],\n",
       "        ...,\n",
       "        [9.9999994e-01, 8.4723510e-08],\n",
       "        [9.9999630e-01, 3.6968522e-06],\n",
       "        [9.9999958e-01, 3.9422011e-07]], dtype=float32),\n",
       " array([[9.9999428e-01, 5.6968356e-06],\n",
       "        [9.9999428e-01, 5.7500188e-06],\n",
       "        [9.9999380e-01, 6.2340646e-06],\n",
       "        ...,\n",
       "        [8.7130073e-05, 9.9991286e-01],\n",
       "        [8.7130073e-05, 9.9991286e-01],\n",
       "        [8.7130073e-05, 9.9991286e-01]], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, stratify = y, random_state = random_seed)\n",
    "\n",
    "# X_train_oversampled, y_train_oversampled = Smoter(X_train, y_train, is_random=True)\n",
    "config = get_toy_config()\n",
    "gc = GCForest(config)\n",
    "\n",
    "gc.fit_transform(X_train, y_train, X_test, y_test)\n",
    "# y_valid_pred = gc.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump\n",
    "with open(\"../pkl/2018_gc.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gc, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# # load\n",
    "# with open(\"../pkl/2018_gc.pkl\", \"rb\") as f:\n",
    "#     gc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
