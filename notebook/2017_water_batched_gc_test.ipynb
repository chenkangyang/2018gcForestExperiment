{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "from gcforest.gcforest import GCForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layering: divide the data into N layers, make sure every layer has the same distribution of 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true, pred):\n",
    "    # compute accuracy, precision and recall\n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "\n",
    "    for i in range(0, len(pred)):\n",
    "        if pred[i] == true[i] and true[i] == 1:\n",
    "            TP += 1\n",
    "        elif pred[i] == true[i] and true[i] == 0:\n",
    "            TN += 1\n",
    "        elif pred[i] != true[i] and true[i] == 0:\n",
    "            FP += 1\n",
    "        elif pred[i] != true[i] and true[i] == 1:\n",
    "            FN += 1\n",
    "    print('TP=',TP,'FP=',FP,'TN=',TN,'FN=',FN)\n",
    "#     print(\"F1\", f1_score(true, pred))\n",
    "    precision = TP/(TP + FP)\n",
    "    recall = TP/(TP + FN)\n",
    "    accuracy = (TP+TN)/(TP+TN+FN+FP)\n",
    "    \n",
    "\n",
    "    F1 = 2*precision*recall / (precision + recall)\n",
    "    print(\"precision\", precision,\"\\nrecall\", recall,\"\\naccuracy\", accuracy)\n",
    "    print('F1=',F1)\n",
    "    return F1, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch\n",
    "\n",
    "combine serveral datasâ€˜ features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch(X, y, size):\n",
    "    batch_size = size\n",
    "\n",
    "    X_trim = X\n",
    "    y_trim = y\n",
    "\n",
    "    if len(X) % batch_size != 0:\n",
    "        extra_num = len(X) % batch_size\n",
    "        X_trim = np.delete(X, range(len(X) - extra_num, len(X)), axis = 0)\n",
    "        y_trim = np.delete(y, range(len(y) - extra_num, len(y)), axis = 0)\n",
    "\n",
    "    X_batch = np.split(X_trim, len(X_trim)/batch_size)\n",
    "    y_batch = np.split(y_trim, len(y_trim)/batch_size)\n",
    "\n",
    "    num_batch = 0\n",
    "\n",
    "    for each_batch in X_batch:\n",
    "        X_batch[num_batch] = np.reshape(X_batch[num_batch], (9*batch_size))\n",
    "        y_batch[num_batch] = y_batch[num_batch][-1]\n",
    "        num_batch += 1\n",
    "\n",
    "    X_batch = np.array(X_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the data before somte\n",
    "\n",
    "fulfill the Na with median, then standardized the data, output type ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gc_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_config():\n",
    "    config = {}\n",
    "    ca_config = {}\n",
    "    ca_config[\"random_state\"] = 42\n",
    "    ca_config[\"max_layers\"] = 10\n",
    "    ca_config[\"early_stopping_rounds\"] = 3\n",
    "    ca_config[\"n_classes\"] = 2\n",
    "    ca_config[\"estimators\"] = []\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"RandomForestClassifier\", \"n_estimators\": 10, \"max_depth\": None, \"n_jobs\": -1})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"ExtraTreesClassifier\", \"n_estimators\": 10, \"max_depth\": None, \"n_jobs\": -1})\n",
    "    ca_config[\"estimators\"].append({\"n_folds\": 5, \"type\": \"LogisticRegression\"})\n",
    "    config[\"cascade\"] = ca_config\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "kf = 100 # N-layers\n",
    "test_size = 0.33\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GcForest\n",
    "\n",
    "## test gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test GcForest on valid datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load 2017 Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"../data/water/txt/2017waterDataTesting.txt\").readlines()\n",
    "num_lines = len(lines) - 1\n",
    "\n",
    "X_test = np.ones((num_lines, 9))\n",
    "y_test = np.ones((num_lines, 1))\n",
    "flag = 0\n",
    "\n",
    "lines = np.delete(lines, 0, axis = 0)\n",
    "i = 0\n",
    "\n",
    "for line in lines:\n",
    "    data_line = line.split()\n",
    "    feature = data_line[3:12]\n",
    "    for k in range(9):\n",
    "        if feature[k] == 'NA':\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "        flag = 0\n",
    "        continue    # jump out of the loop\n",
    "    X_test[i] = feature    \n",
    "    if data_line[12] == 'FALSE':\n",
    "        y_test[i] = 0\n",
    "    elif data_line[12] == 'TRUE':\n",
    "        y_test[i] = 1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "X_test = clean_pipeline.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. test gcForest on 2017 Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-11-28 15:41:44,142][cascade_classifier.transform] X_groups_test.shape=[(244668, 9)]\n",
      "[ 2018-11-28 15:41:44,147][cascade_classifier.transform] group_dims=[9]\n",
      "[ 2018-11-28 15:41:44,148][cascade_classifier.transform] X_test.shape=(244668, 9)\n",
      "[ 2018-11-28 15:41:44,151][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(244668, 9)\n",
      "[ 2018-11-28 15:41:45,490][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(244668, 15)\n",
      "[ 2018-11-28 15:41:46,683][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(244668, 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= 2017 datasets' results on test =============\n",
      "TP= 58194 FP= 0 TN= 183718 FN= 2756\n",
      "precision 1.0 \n",
      "recall 0.9547826086956521 \n",
      "accuracy 0.988735756208413\n",
      "F1= 0.9768683274021353\n"
     ]
    }
   ],
   "source": [
    "with open(\"../pkl/2017_test.pkl\", \"rb\") as f:\n",
    "    gc = pickle.load(f)\n",
    "    y_test_pred = gc.predict(X_test)\n",
    "    print(\"============= 2017 datasets' results on test =============\")\n",
    "    evaluate(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. test GcForest on 2017 batched Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-11-28 15:41:49,464][cascade_classifier.transform] X_groups_test.shape=[(81556, 27)]\n",
      "[ 2018-11-28 15:41:49,473][cascade_classifier.transform] group_dims=[27]\n",
      "[ 2018-11-28 15:41:49,474][cascade_classifier.transform] X_test.shape=(81556, 27)\n",
      "[ 2018-11-28 15:41:49,486][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(81556, 27)\n",
      "[ 2018-11-28 15:41:50,648][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(81556, 33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= 2017 datasets' results on 3 batched test =============\n",
      "TP= 19394 FP= 0 TN= 61238 FN= 924\n",
      "precision 1.0 \n",
      "recall 0.9545230829806083 \n",
      "accuracy 0.9886703614694198\n",
      "F1= 0.9767324738114425\n"
     ]
    }
   ],
   "source": [
    "X_test_batch, y_test_batch = Batch(X_test, y_test, batch_size)\n",
    "    \n",
    "with open(\"../pkl/2017_test_batch.pkl\", \"rb\") as f:\n",
    "    gc = pickle.load(f)\n",
    "    y_test_pred = gc.predict(X_test_batch)\n",
    "    print(\"============= 2017 datasets' results on %d batched test =============\" %(batch_size))\n",
    "    evaluate(y_test_batch, y_test_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
