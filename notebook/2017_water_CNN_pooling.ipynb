{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layering: divide the data into N layers, make sure every layer has the same distribution of 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Layering(df, N):\n",
    "    new_data=df.iloc[:,0:]\n",
    "\n",
    "    data_maj = new_data[new_data['EVENT']==0]\n",
    "    data_min = new_data[new_data['EVENT']==1]\n",
    "    n_maj=data_maj.iloc[:,0].size\n",
    "    n_min=data_min.iloc[:,0].size\n",
    "    M1=n_maj%N\n",
    "    M2=n_min%N\n",
    "    stepD=int(n_maj/10)\n",
    "    stepS=int(n_min/10)\n",
    "\n",
    "    maj_data = []\n",
    "    for i in range(N):\n",
    "        maj_data.append(data_maj.iloc[i*stepD:(i+1)*stepD])\n",
    "    for i in range(M1):\n",
    "        maj_data[i]=maj_data[i].append(data_maj.iloc[stepD*N+i:stepD*N+i+1])\n",
    "\n",
    "\n",
    "    min_data = []\n",
    "    for i in range(N):\n",
    "        min_data.append(data_min.iloc[i*stepS:(i+1)*stepS])\n",
    "    for i in range(M2):\n",
    "        min_data[i]=min_data[i].append(data_min.iloc[stepS*N+i:stepS*N+i+1])\n",
    "\n",
    "    Last_Data = pd.DataFrame()\n",
    "    for i in range(N):\n",
    "        Last_Data=Last_Data.append(maj_data[i].append(min_data[i]))\n",
    "    return Last_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### somte sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Smoter(X, y, is_random=False):\n",
    "    if is_random == True:\n",
    "        random_lst = list(np.random.randint(0, 1000, 4))\n",
    "    elif is_random == False:\n",
    "        random_lst = [0] * 4\n",
    "\n",
    "    sm = SMOTE(random_state=random_lst[2])\n",
    "    X_smote, y_smote = sm.fit_sample(X, y)\n",
    "    y_smote = y_smote[:,np.newaxis]\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(v_xs, v_ys, sess):\n",
    "    global prediction\n",
    "    \n",
    "    predicted = tf.argmax(prediction, 1)\n",
    "    actual = np.argmax(v_ys, 1)\n",
    "\n",
    "    # Count true positives, true negatives, false positives and false negatives.\n",
    "    tp_op = tf.count_nonzero(predicted * actual)\n",
    "    tn_op = tf.count_nonzero((predicted - 1) * (actual - 1))\n",
    "    fp_op = tf.count_nonzero(predicted * (actual - 1))\n",
    "    fn_op = tf.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "    tp, tn, fp, fn = \\\n",
    "    sess.run(\n",
    "        [tp_op, tn_op, fp_op, fn_op], \n",
    "        feed_dict={xs: v_xs, ys: v_ys, keep_prob:1}\n",
    "    )\n",
    "#     print('TP=',tp,'FP=',fp,'TN=',tn,'FN=',fn)\n",
    "    # Calculate accuracy, precision, recall and F1 score.\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "#     print('Precision = ', precision)\n",
    "#     print('Recall = ', recall)\n",
    "#     print('F1 Score = ', f1_score)\n",
    "#     print('Accuracy = ', accuracy)\n",
    "    return precision, recall, f1_score, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size = 100):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,name=None):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    if name is None:\n",
    "        return tf.Variable(initial)\n",
    "    else:\n",
    "        return tf.Variable(initial,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    if name is None:\n",
    "        return tf.Variable(initial)\n",
    "    else:\n",
    "        return tf.Variable(initial,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(x, W):\n",
    "    return tf.nn.conv1d(x, W, stride=stride_num, padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = 1\n",
    "stride_num = 1\n",
    "pool_patch_size = 2\n",
    "kf = 100\n",
    "lr = 0.1\n",
    "max_iterations = 101\n",
    "hidden_cell_num = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load 2017 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('../data/water/txt/2017waterDataTraining.txt',delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "Time = np.zeros(df.shape[0]).astype(\"str\")\n",
    "for i in range(len(df)):\n",
    "    Time[i] = df['index'][i]+\" \"+ df['Time'][i]\n",
    "df['Time'] = Time\n",
    "df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering\n",
    "\n",
    "\n",
    "It looks like we have 14 columns to help us predict our classification. We will drop fnlwgt and education and then convert our categorical features to dummy variables. We will also convert our label to 0 and 1 where 1 means the person made more than $50k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Time']\n",
    "continuous_features = ['Tp', 'Cl', 'pH', 'Redox', 'Leit', 'Trueb', 'Cl_2', 'Fm', 'Fm_2']\n",
    "cat_features =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_dummies = pd.get_dummies(df, columns=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_dummies.drop(drop_columns, 1, inplace=True)\n",
    "# delte NA datas\n",
    "all_df_dummies = all_df_dummies.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = all_df_dummies.drop(['EVENT'], axis=1) # Series\n",
    "y_train = all_df_dummies['EVENT'].apply(lambda x: 0 if x == False else 1) # Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train,y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tp</th>\n",
       "      <th>Cl</th>\n",
       "      <th>pH</th>\n",
       "      <th>Redox</th>\n",
       "      <th>Leit</th>\n",
       "      <th>Trueb</th>\n",
       "      <th>Cl_2</th>\n",
       "      <th>Fm</th>\n",
       "      <th>Fm_2</th>\n",
       "      <th>EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.110</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1471.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.37</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>8.38</td>\n",
       "      <td>755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.111</td>\n",
       "      <td>1476.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tp    Cl    pH  Redox   Leit  Trueb   Cl_2      Fm    Fm_2  EVENT\n",
       "0  4.4  0.14  8.38  755.0  232.0  0.009  0.110  1428.0  1020.0      0\n",
       "1  4.4  0.14  8.38  755.0  232.0  0.009  0.111  1436.0  1018.0      0\n",
       "2  4.4  0.14  8.38  755.0  232.0  0.014  0.113  1471.0  1019.0      0\n",
       "3  4.4  0.14  8.37  755.0  232.0  0.015  0.111  1457.0  1015.0      0\n",
       "4  4.4  0.14  8.38  755.0  232.0  0.013  0.111  1476.0  1019.0      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layer sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ layer sampling ============\n"
     ]
    }
   ],
   "source": [
    "print(\"============ layer sampling ============\")\n",
    "train_layer = Layering(train, kf)\n",
    "array = train_layer.values\n",
    "X_train = array[:, 0:-1] # ndarray\n",
    "y_train = array[:, -1] # ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do somte sampling on the train data to solve data imblance problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ SMOTE ============\n",
      "train: 110812, contains 0.9843 of 0 , after SMOTE: train: 218144 contains 0.5000 of 1\n"
     ]
    }
   ],
   "source": [
    "X_train_oversampled, y_train_oversampled = Smoter(X_train, y_train, is_random=True)\n",
    "print(\"============ SMOTE ============\")\n",
    "print(\"train: %d, contains %.4f of 0 , after SMOTE: train: %d contains %.4f of 1\" %(X_train.shape[0], (y_train == 0).sum()/y_train.shape[0], X_train_oversampled.shape[0], (y_train_oversampled == 0).sum()/y_train_oversampled.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the data before somte\n",
    "\n",
    "fulfill the Na with median, then standardized the data, output type ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pipeline = Pipeline([('imputer', preprocessing.Imputer(missing_values='NaN',strategy=\"median\")),\n",
    "                           ('std_scaler', preprocessing.StandardScaler()),])\n",
    "X_train_oversampled = clean_pipeline.fit_transform(X_train_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfer y into probability vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_oversampled_pro = np.zeros([y_train_oversampled.shape[0], 2])\n",
    "for i in range(len(y_train_oversampled)):\n",
    "    if y_train_oversampled[i] == 1:\n",
    "        y_train_oversampled_pro[i] = np.array([0, 1])\n",
    "    else:\n",
    "        y_train_oversampled_pro[i] = np.array([1, 0])\n",
    "y_train_oversampled = y_train_oversampled_pro        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load 2017 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"../data/water/txt/2017waterDataTesting.txt\").readlines()\n",
    "num_lines = len(lines) - 1\n",
    "\n",
    "X_test = np.ones((num_lines, 9))\n",
    "y_test = np.ones((num_lines, 1))\n",
    "flag = 0\n",
    "\n",
    "lines = np.delete(lines, 0, axis = 0)\n",
    "i = 0\n",
    "\n",
    "for line in lines:\n",
    "    data_line = line.split()\n",
    "    feature = data_line[3:12]\n",
    "    for k in range(9):\n",
    "        if feature[k] == 'NA':\n",
    "            flag = 1\n",
    "            break\n",
    "    if flag == 1:\n",
    "        flag = 0\n",
    "        continue    # jump out of the loop\n",
    "    X_test[i] = feature    \n",
    "    if data_line[12] == 'FALSE':\n",
    "        y_test[i] = 0\n",
    "    elif data_line[12] == 'TRUE':\n",
    "        y_test[i] = 1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "X_test = clean_pipeline.transform(X_test) \n",
    "\n",
    "y_test_pro = np.zeros([y_test.shape[0], 2])\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 1:\n",
    "        y_test_pro[i] = np.array([0, 1])\n",
    "    else:\n",
    "        y_test_pro[i] = np.array([1, 0])\n",
    "y_test = y_test_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造网络\n",
    "两个（卷积+最大池化），两个全联接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define placeholder for inputs to network\n",
    "# keep_prob = tf.placeholder(tf.float32)\n",
    "# xs = tf.placeholder(tf.float32, [None, 9])\n",
    "# ys = tf.placeholder(tf.float32, [None, 2])\n",
    "# learning_rate = tf.placeholder(tf.float32)\n",
    "# X_input = tf.reshape(xs,[-1,9,1]) # [n_samples, 9 ,1]    -1 具体是多少由导入数据决定（多少组数据） \n",
    "    \n",
    "# def my_cnn():\n",
    "    \n",
    "#     ## conv1 layer ##\n",
    "#     W_conv1 = weight_variable([3,1,6],name=\"W_conv1\") # patch: 3, in size 1(通道数), out size 6（feature_map数量，一个卷积核生成一个feature_map）\n",
    "#     b_conv1 = bias_variable([6],name=\"b_conv1\")\n",
    "#     h_conv1 = tf.nn.relu(conv1d(X_input, W_conv1) + b_conv1) # output size 1*9*6\n",
    "#     print(\"h_conv1\", h_conv1.shape)\n",
    "#     ## poo1 layer ##\n",
    "#     max_pool_1 = tf.layers.max_pooling1d(inputs=h_conv1, pool_size=pool_patch_size, strides=stride_num, padding='same')\n",
    "#     print(\"max_pool_1\", max_pool_1.shape)\n",
    "\n",
    "#     ## conv2 layer ##\n",
    "#     W_conv2 = weight_variable([3,6,12],name=\"W_conv2\") # patch: 3, in size 6，out size 12\n",
    "#     b_conv2 = bias_variable([12],name=\"b_conv2\")\n",
    "#     h_conv2 = tf.nn.relu(conv1d(max_pool_1, W_conv2) + b_conv2) # output size 1*9*12\n",
    "#     print(\"h_conv2\", h_conv2.shape)\n",
    "#     ## poo2 layer ##\n",
    "#     max_pool_2 = tf.layers.max_pooling1d(inputs=h_conv2, pool_size=pool_patch_size, strides=stride_num, padding='same')\n",
    "#     print(\"max_pool_2\", max_pool_2.shape)\n",
    "\n",
    "#     ## func1 layer ##\n",
    "#     W_fc1 = weight_variable([9*12,hidden_cell_num],name=\"W_fc1\")\n",
    "#     b_fc1 = bias_variable([hidden_cell_num],name=\"b_fc1\")\n",
    "\n",
    "#     max_pool_2_flat = tf.reshape(max_pool_2, [-1,9*12])\n",
    "#     h_fc1 = tf.nn.relu(tf.matmul(max_pool_2_flat, W_fc1)+b_fc1)\n",
    "#     h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#     print(\"h_fc1_drop\", h_fc1_drop.shape)\n",
    "\n",
    "#     ## func2 layer ##\n",
    "#     W_fc2 = weight_variable([hidden_cell_num,2],name=\"W_fc2\")\n",
    "#     b_fc2 = bias_variable([1],name=\"b_fc2\")\n",
    "#     prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "#     print(\"prediction\", prediction.shape)\n",
    "\n",
    "#     var_dict = {'W_conv1': W_conv1, \n",
    "#                 'b_conv1': b_conv1, \n",
    "#                 'W_conv2': W_conv2, \n",
    "#                 'b_conv2': b_conv2, \n",
    "#                 'W_fc1': W_fc1, \n",
    "#                 'b_fc1': b_fc1,\n",
    "#                 'W_fc2': W_fc2,\n",
    "#                 'b_fc2': b_fc2}\n",
    "#     return prediction, var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholder for inputs to network\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "xs = tf.placeholder(tf.float32, [None, 9])\n",
    "ys = tf.placeholder(tf.float32, [None, 2])\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "X_input = tf.reshape(xs,[-1,9,1]) # [n_samples, 9 ,1]    -1 具体是多少由导入数据决定（多少组数据） \n",
    "    \n",
    "def my_cnn():\n",
    "    \n",
    "   ## conv1 layer ##\n",
    "    W_conv1 = weight_variable([3,1,6],name=\"W_conv1\") # patch: 3, in size 1(通道数), out size 6（feature_map数量，一个卷积核生成一个feature_map）\n",
    "    b_conv1 = bias_variable([6],name=\"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(conv1d(X_input, W_conv1) + b_conv1) # output size 1*9*6\n",
    "    print(h_conv1.shape)\n",
    "\n",
    "    ## conv2 layer ##\n",
    "    W_conv2 = weight_variable([3,6,12],name=\"W_conv2\") # patch: 3, in size 6，out size 12\n",
    "    b_conv2 = bias_variable([12],name=\"b_conv2\")\n",
    "    h_conv2 = tf.nn.relu(conv1d(h_conv1, W_conv2) + b_conv2) # output size 1*9*12\n",
    "    print(h_conv2.shape)\n",
    "\n",
    "    ## func1 layer ##\n",
    "    W_fc1 = weight_variable([9*12,hidden_cell_num],name=\"W_fc1\")\n",
    "    b_fc1 = bias_variable([hidden_cell_num],name=\"b_fc1\")\n",
    "\n",
    "    h_conv2_flat = tf.reshape(h_conv2, [-1,9*12])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1)+b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    print(h_fc1_drop.shape)\n",
    "\n",
    "    ## func2 layer ##\n",
    "    W_fc2 = weight_variable([hidden_cell_num,2],name=\"W_fc2\")\n",
    "    b_fc2 = bias_variable([1],name=\"b_fc2\")\n",
    "    prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "    print(prediction.shape)\n",
    "\n",
    "    var_dict = {'W_conv1': W_conv1, \n",
    "                'b_conv1': b_conv1, \n",
    "                'W_conv2': W_conv2, \n",
    "                'b_conv2': b_conv2, \n",
    "                'W_fc1': W_fc1, \n",
    "                'b_fc1': b_fc1,\n",
    "                'W_fc2': W_fc2,\n",
    "                'b_fc2': b_fc2}\n",
    "    return prediction, var_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 9, 6)\n",
      "(?, 9, 12)\n",
      "(?, 64)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "prediction, var_dict = my_cnn()\n",
    "\n",
    "# the error between prediction and real data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=ys))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/envs/gc/lib/python3.6/site-packages/ipykernel/__main__.py:20: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0/101 Train loss: 0.792068 Test loss: 0.568591 Test acc: 0.750887 Test f1: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d9b81a1f085e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_oversampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_oversampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0d18e2b0f8f1>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(v_xs, v_ys, sess)\u001b[0m\n\u001b[1;32m     13\u001b[0m     tp, tn, fp, fn =     sess.run(\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtp_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     print('TP=',tp,'FP=',fp,'TN=',tn,'FN=',fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1330\u001b[0m                 run_metadata):\n\u001b[1;32m   1331\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1332\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gc/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m           tf_session.TF_ExtendGraph(self._session,\n\u001b[0;32m-> 1392\u001b[0;31m                                     graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "test_f1 = []\n",
    "test_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_f1 = []\n",
    "train_loss = []\n",
    "\n",
    "X_train_oversampled = np.array(X_train_oversampled, dtype=np.float32)\n",
    "y_train_oversampled = np.array(y_train_oversampled, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "    for i in range(max_iterations):\n",
    "        feed = {xs : X_train_oversampled, ys : y_train_oversampled, keep_prob : kp, learning_rate : lr}\n",
    "        sess.run(train_op, feed_dict=feed)\n",
    "        \n",
    "        loss, _ = sess.run([cost, train_op], feed_dict=feed)\n",
    "        precision, recall, f1_score, accuracy = evaluate(X_train_oversampled, y_train_oversampled, sess)\n",
    "        \n",
    "        train_loss.append(loss) \n",
    "        train_f1.append(f1_score)\n",
    "        train_acc.append(accuracy)\n",
    "        if i % 50 ==0:\n",
    "            loss_t = sess.run(cost, feed_dict={xs:X_test , ys:y_test , keep_prob: kp})\n",
    "            test_loss.append(loss_t)\n",
    "            test_precision, test_recall, test_f1_score, test_accuracy = evaluate(X_test, y_test, sess)\n",
    "            test_f1.append(test_f1_score)\n",
    "            test_acc.append(test_accuracy)\n",
    "            print(\"Iteration: {}/{}\".format(i, max_iterations),\n",
    "                  \"Train loss: {:6f}\".format(loss),\n",
    "                  \"Test loss: {:6f}\".format(loss_t),\n",
    "                  \"Test acc: {:.6f}\".format(test_accuracy),\n",
    "                  \"Test f1: {:.6f}\".format(test_f1_score))\n",
    "            \n",
    "    saver = tf.train.Saver(var_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAF3CAYAAABUsGfpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(max_iterations)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 50 == 0], np.array(test_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 50 == 0], test_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_f1), 'r-', t[t % 50 == 0], test_f1, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = saver.save(sess,\"cnn_2017/2017_save_net.ckpt\")\n",
    "# print(\"Save to path:\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gggg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " ...\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "actual [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "TP= 218144 FP= 218144 TN= 0 FN= 0\n",
      "Precision =  0.5\n",
      "Recall =  1.0\n",
      "F1 Score =  0.6666666666666666\n",
      "Accuracy =  0.5\n"
     ]
    }
   ],
   "source": [
    "# predicted = tf.argmax(prediction, 1)\n",
    "# actual = tf.argmax(ys, 1)\n",
    "\n",
    "predicted = tf.round(tf.nn.sigmoid(prediction))\n",
    "actual = ys\n",
    "\n",
    "# Count true positives, true negatives, false positives and false negatives.\n",
    "tp_op = tf.count_nonzero(predicted * actual)\n",
    "tn_op = tf.count_nonzero((predicted - 1) * (actual - 1))\n",
    "fp_op = tf.count_nonzero(predicted * (actual - 1))\n",
    "fn_op = tf.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed = {xs : X_train_oversampled, ys : y_train_oversampled, keep_prob : 1, learning_rate : lr}\n",
    "    tp, tn, fp, fn, pr, ac= \\\n",
    "    sess.run(\n",
    "        [tp_op, tn_op, fp_op, fn_op, predicted, actual], \n",
    "        feed_dict=feed\n",
    "    )\n",
    "\n",
    "    print(\"predicted\", pr)\n",
    "    print(\"actual\", ac)\n",
    "    \n",
    "    print('TP=',tp,'FP=',fp,'TN=',tn,'FN=',fn)\n",
    "    \n",
    "    # Calculate accuracy, precision, recall and F1 score.\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    print('Precision = ', precision)\n",
    "    print('Recall = ', recall)\n",
    "    print('F1 Score = ', f1_score)\n",
    "    print('Accuracy = ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted [1 1 1 ... 1 1 1]\n",
      "actual [0 0 0 ... 1 1 1]\n",
      "TP= 102635 FP= 109071 TN= 1 FN= 6437\n",
      "Precision =  0.4847996750210197\n",
      "Recall =  0.9409839372157841\n",
      "F1 Score =  0.6399129616120806\n",
      "Accuracy =  0.47049655273580754\n"
     ]
    }
   ],
   "source": [
    "predicted = tf.argmax(prediction, 1)\n",
    "actual = tf.argmax(ys, 1)\n",
    "\n",
    "\n",
    "# Count true positives, true negatives, false positives and false negatives.\n",
    "tp_op = tf.count_nonzero(predicted * actual)\n",
    "tn_op = tf.count_nonzero((predicted - 1) * (actual - 1))\n",
    "fp_op = tf.count_nonzero(predicted * (actual - 1))\n",
    "fn_op = tf.count_nonzero((predicted - 1) * actual)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed = {xs : X_train_oversampled, ys : y_train_oversampled, keep_prob : 1, learning_rate : lr}\n",
    "    tp, tn, fp, fn, pr, ac= \\\n",
    "    sess.run(\n",
    "        [tp_op, tn_op, fp_op, fn_op, predicted, actual], \n",
    "        feed_dict=feed\n",
    "    )\n",
    "\n",
    "    print(\"predicted\", pr)\n",
    "    print(\"actual\", ac)\n",
    "    \n",
    "    print('TP=',tp,'FP=',fp,'TN=',tn,'FN=',fn)\n",
    "    \n",
    "    # Calculate accuracy, precision, recall and F1 score.\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    print('Precision = ', precision)\n",
    "    print('Recall = ', recall)\n",
    "    print('F1 Score = ', f1_score)\n",
    "    print('Accuracy = ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gc]",
   "language": "python",
   "name": "conda-env-gc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
